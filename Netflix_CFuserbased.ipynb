{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Based Collaborative Filtering Recommender System - Netflix\n",
    "\n",
    "**Nadia Aflatounian**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This Jupyter notebook focuses on the implementation of User-Based Collaborative Filtering (UBCF) using K-Nearest Neighbors (KNN) prediction and KNN classification techniques on the **Netflix dataset**. The primary goal is to address the challenges posed by the dataset's high sparsity, reaching around 98%, by exploring different methods for calculating similarities and optimizing the recommendation process accordingly.\n",
    "\n",
    "**Key Objectives**:\n",
    "- Implement User-Based Collaborative Filtering tailored to the high sparsity of the Netflix dataset.\n",
    "- Investigate various similarity metrics and neighbor distances to enhance recommendation accuracy.\n",
    "- Develop robust logics for both rating prediction and classification tasks.\n",
    "\n",
    "**Methods Implemented**:\n",
    "1. **KNN Prediction**: Predicting ratings based on user-item interactions while considering the unique characteristics of the Netflix dataset.\n",
    "2. **KNN Classification**: Classifying users into groups based on their preferences, leveraging KNN principles adapted to the dataset's properties.\n",
    "\n",
    "**Approach**:\n",
    "- Given the high sparsity of the Netflix dataset, special attention is paid to methods that can effectively handle sparse data.\n",
    "\n",
    "**Outcome**:\n",
    "By the end of this notebook, insights into the effectiveness of various similarity measures and neighbor distance metrics specific to the Netflix dataset will be gained. Additionally, well-defined logics for predicting ratings and classifying users will be established, contributing to the enhancement of the recommendation system's performance in the context of the Netflix dataset's unique challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>YearOfRelease</th>\n",
       "      <th>MovieTitle</th>\n",
       "      <th>RatingYear</th>\n",
       "      <th>MovieAge</th>\n",
       "      <th>user_activity</th>\n",
       "      <th>AverageMovieAgeRated</th>\n",
       "      <th>user_average_rating</th>\n",
       "      <th>average_rating_per_movie</th>\n",
       "      <th>number_of_ratings_per_movie</th>\n",
       "      <th>scaled_movie_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1488844</td>\n",
       "      <td>3</td>\n",
       "      <td>2005-09-06</td>\n",
       "      <td>2003</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>1.473012</td>\n",
       "      <td>1.640503</td>\n",
       "      <td>3.253308</td>\n",
       "      <td>3.910534</td>\n",
       "      <td>1.010541</td>\n",
       "      <td>1.215054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>822109</td>\n",
       "      <td>5</td>\n",
       "      <td>2005-05-13</td>\n",
       "      <td>2003</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>1.031355</td>\n",
       "      <td>1.405855</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>3.910534</td>\n",
       "      <td>1.010541</td>\n",
       "      <td>1.215054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>885013</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-10-19</td>\n",
       "      <td>2003</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>1.077044</td>\n",
       "      <td>1.400853</td>\n",
       "      <td>3.873563</td>\n",
       "      <td>3.910534</td>\n",
       "      <td>1.010541</td>\n",
       "      <td>1.215054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30878</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-12-26</td>\n",
       "      <td>2003</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>1.275924</td>\n",
       "      <td>1.525706</td>\n",
       "      <td>3.634304</td>\n",
       "      <td>3.910534</td>\n",
       "      <td>1.010541</td>\n",
       "      <td>1.215054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>823519</td>\n",
       "      <td>3</td>\n",
       "      <td>2004-05-03</td>\n",
       "      <td>2003</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>1.139754</td>\n",
       "      <td>1.326786</td>\n",
       "      <td>3.917197</td>\n",
       "      <td>3.910534</td>\n",
       "      <td>1.010541</td>\n",
       "      <td>1.172043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID  CustomerID  Rating        Date  YearOfRelease       MovieTitle  \\\n",
       "0        1     1488844       3  2005-09-06           2003  Dinosaur Planet   \n",
       "1        1      822109       5  2005-05-13           2003  Dinosaur Planet   \n",
       "2        1      885013       4  2005-10-19           2003  Dinosaur Planet   \n",
       "3        1       30878       4  2005-12-26           2003  Dinosaur Planet   \n",
       "4        1      823519       3  2004-05-03           2003  Dinosaur Planet   \n",
       "\n",
       "   RatingYear  MovieAge  user_activity  AverageMovieAgeRated  \\\n",
       "0        2005         2       1.473012              1.640503   \n",
       "1        2005         2       1.031355              1.405855   \n",
       "2        2005         2       1.077044              1.400853   \n",
       "3        2005         2       1.275924              1.525706   \n",
       "4        2004         1       1.139754              1.326786   \n",
       "\n",
       "   user_average_rating  average_rating_per_movie  number_of_ratings_per_movie  \\\n",
       "0             3.253308                  3.910534                     1.010541   \n",
       "1             4.083333                  3.910534                     1.010541   \n",
       "2             3.873563                  3.910534                     1.010541   \n",
       "3             3.634304                  3.910534                     1.010541   \n",
       "4             3.917197                  3.910534                     1.010541   \n",
       "\n",
       "   scaled_movie_age  \n",
       "0          1.215054  \n",
       "1          1.215054  \n",
       "2          1.215054  \n",
       "3          1.215054  \n",
       "4          1.172043  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "training_df = pd.read_csv('C:/Users/nafla/OneDrive/Documents/system development/training_data.csv')\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25     8.0\n",
      "0.50    24.0\n",
      "0.75    64.0\n",
      "Name: count, dtype: float64\n",
      "0.25     192.0\n",
      "0.50     552.5\n",
      "0.75    2539.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate quantiles for user activity and item popularity\n",
    "user_activity_quantiles = training_df['CustomerID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "item_popularity_quantiles = training_df['MovieID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "print(user_activity_quantiles)\n",
    "print(item_popularity_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting column types\n",
    "training_df['CustomerID'] = training_df['CustomerID'].astype(str)\n",
    "training_df['MovieID'] = training_df['MovieID'].astype(str)\n",
    "training_df['Rating'] = pd.to_numeric(training_df['Rating'], errors='coerce')  # Converts to float, makes non-numeric as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MovieID CustomerID  Rating        Date  YearOfRelease       MovieTitle  \\\n",
      "0       1    1488844       3  2005-09-06           2003  Dinosaur Planet   \n",
      "1       1     822109       5  2005-05-13           2003  Dinosaur Planet   \n",
      "2       1     885013       4  2005-10-19           2003  Dinosaur Planet   \n",
      "3       1      30878       4  2005-12-26           2003  Dinosaur Planet   \n",
      "4       1     823519       3  2004-05-03           2003  Dinosaur Planet   \n",
      "\n",
      "   RatingYear  user_activity  AverageMovieAgeRated  user_average_rating  \\\n",
      "0        2005              1                     2                    3   \n",
      "1        2005              1                     1                    4   \n",
      "2        2005              1                     1                    4   \n",
      "3        2005              1                     2                    4   \n",
      "4        2004              1                     1                    4   \n",
      "\n",
      "   scaled_movie_age  \n",
      "0          1.215054  \n",
      "1          1.215054  \n",
      "2          1.215054  \n",
      "3          1.215054  \n",
      "4          1.172043  \n"
     ]
    }
   ],
   "source": [
    "# List of your columns to be rounded and converted\n",
    "columns_to_round_and_convert = ['user_activity', 'AverageMovieAgeRated', 'user_average_rating']\n",
    "\n",
    "# when trying to create similarity matrix containing user feature we ran to memory error\n",
    "# Apply rounding and conversion to all specified columns to use less memory \n",
    "for column in columns_to_round_and_convert:\n",
    "    training_df[column] = training_df[column].round().astype(int)\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "print(training_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MovieID CustomerID  Rating        Date  YearOfRelease       MovieTitle  \\\n",
      "0       1    1488844       3  2005-09-06           2003  Dinosaur Planet   \n",
      "1       1     822109       5  2005-05-13           2003  Dinosaur Planet   \n",
      "2       1     885013       4  2005-10-19           2003  Dinosaur Planet   \n",
      "3       1      30878       4  2005-12-26           2003  Dinosaur Planet   \n",
      "4       1     823519       3  2004-05-03           2003  Dinosaur Planet   \n",
      "\n",
      "   RatingYear  user_activity  AverageMovieAgeRated  user_average_rating  \\\n",
      "0        2005              1                     2                    3   \n",
      "1        2005              1                     1                    4   \n",
      "2        2005              1                     1                    4   \n",
      "3        2005              1                     2                    4   \n",
      "4        2004              1                     1                    4   \n",
      "\n",
      "   scaled_movie_age  \n",
      "0          1.215054  \n",
      "1          1.215054  \n",
      "2          1.215054  \n",
      "3          1.215054  \n",
      "4          1.172043  \n"
     ]
    }
   ],
   "source": [
    "# drop columns we don't need\n",
    "# List the names of the columns you want to drop\n",
    "columns_to_drop = ['average_rating_per_movie', 'number_of_ratings_per_movie', 'MovieAge']\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "training_df = training_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "print(training_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Strategy\n",
    "\n",
    "To address the high sparsity and memory constraints in the dataset, a filtering and sampling approach is applied. This involves several steps:\n",
    "\n",
    "1. **Filtering Users and Movies**: Users and movies with fewer than 10 ratings are filtered out, aiming to reduce the dataset's sparsity and focus on more active users and popular movies.\n",
    "\n",
    "2. **Random Sampling**: A random sampling technique is employed to further reduce the dataset size while retaining representative samples. The fraction of data to be sampled is specified as a parameter, with careful consideration for memory limitations.\n",
    "\n",
    "### Note:\n",
    "\n",
    "It's essential to acknowledge that this sampling strategy may result in the loss of some important trends or data patterns present in the original dataset. Additionally, it's recognized that the trained recommender system may not perform similarly on other datasets due to the reduced diversity resulting from the sampling process.\n",
    "\n",
    "By implementing this sampling strategy, the goal is to strike a balance between reducing sparsity and memory usage while maintaining a dataset size that allows for effective training and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (24053575, 11)\n",
      "User-Filtered DataFrame shape: (23343305, 11)\n",
      "Movie-Filtered DataFrame shape: (23343305, 11)\n",
      "Sampled DataFrame shape: (116717, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Filter users with more than 10 ratings\n",
    "user_filtered_df = training_df.groupby('CustomerID').filter(lambda x: len(x) > 10)\n",
    "\n",
    "# Step 2: Filter movies with more than 10 ratings\n",
    "movie_filtered_df = user_filtered_df.groupby('MovieID').filter(lambda x: len(x) > 10)\n",
    "\n",
    "# Step 3: Perform random sampling\n",
    "# Replace 'fraction' with the fraction of data you want to sample. For example, 0.005 for 0.5%\n",
    "fraction = 0.005\n",
    "strat_sample_df = movie_filtered_df.sample(frac=fraction, random_state=42)  # Ensure reproducibility with random_state\n",
    "\n",
    "# Display the shapes of the original, user-filtered, movie-filtered, and sampled DataFrames\n",
    "print(\"Original DataFrame shape:\", training_df.shape)\n",
    "print(\"User-Filtered DataFrame shape:\", user_filtered_df.shape)\n",
    "print(\"Movie-Filtered DataFrame shape:\", movie_filtered_df.shape)\n",
    "print(\"Sampled DataFrame shape:\", strat_sample_df.shape)\n",
    "\n",
    "# 'strat_sample_df' now contains the randomly sampled data from both the users and movies with more than 10 ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the sampled DataFrame: 116717\n"
     ]
    }
   ],
   "source": [
    "num_sampled_rows = len(strat_sample_df)\n",
    "print(f\"Number of rows in the sampled DataFrame: {num_sampled_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset to training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define user_ids to map to their index\n",
    "user_ids = strat_sample_df['CustomerID'].unique()\n",
    "movie_ids = strat_sample_df['MovieID'].unique()\n",
    "\n",
    "# Create mappings based on the entire dataset\n",
    "user_id_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "movie_id_to_index = {movie_id: index for index, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "# spliting data to train and test\n",
    "# later in k-fold validation we create validation set\n",
    "training_data, testing_data = train_test_split(strat_sample_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map index of users and movies for any data set in ordere to use in creating csr matrix and avoid inconsistencies between indices\n",
    "def map_ids_to_indices(df, user_id_to_index, movie_id_to_index):\n",
    "    \"\"\"\n",
    "    Map user IDs and movie IDs to their respective indices.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing 'CustomerID', 'MovieID', and other columns.\n",
    "    - user_id_to_index: Dictionary mapping user IDs to indices.\n",
    "    - movie_id_to_index: Dictionary mapping movie IDs to indices.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with added columns 'UserIndex' and 'MovieIndex' for the respective indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    modified_data = df.copy()\n",
    "    \n",
    "    # Map 'CustomerID' to 'UserIndex'\n",
    "    modified_data['UserIndex'] = modified_data['CustomerID'].map(user_id_to_index)\n",
    "    \n",
    "    # Map 'MovieID' to 'MovieIndex'\n",
    "    modified_data['MovieIndex'] = modified_data['MovieID'].map(movie_id_to_index)\n",
    "    \n",
    "    # Optional: drop rows where either UserIndex or MovieIndex is NaN (i.e., ID wasn't found)\n",
    "    modified_data.dropna(subset=['UserIndex', 'MovieIndex'], inplace=True)\n",
    "    \n",
    "    # Convert indices to integers (they might be floats due to NaN handling)\n",
    "    modified_data['UserIndex'] = modified_data['UserIndex'].astype(int)\n",
    "    modified_data['MovieIndex'] = modified_data['MovieIndex'].astype(int)\n",
    "    \n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping training set to their index\n",
    "mapped_training_data = map_ids_to_indices(training_data,user_id_to_index, movie_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 93373\n",
      "Training Data Size: 93373\n",
      "Testing Data Size: 23344\n"
     ]
    }
   ],
   "source": [
    "# Calculate the size of each split\n",
    "training_size = training_data.shape[0]  # Number of rows in the training data\n",
    "training_size_mapp = mapped_training_data.shape[0]  # Number of rows in the training data\n",
    "testing_size = testing_data.shape[0]  # Number of rows in the testing data\n",
    "\n",
    "# Print the sizes\n",
    "print(f\"Training Data Size: {training_size}\")\n",
    "print(f\"Training Data Size: {training_size_mapp}\")\n",
    "print(f\"Testing Data Size: {testing_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique CustomerIDs in Training Data: 71886\n",
      "Unique MovieIDs in Testing Data: 21626\n"
     ]
    }
   ],
   "source": [
    "# Count unique MovieIDs in the final training data\n",
    "unique_users_training = mapped_training_data['CustomerID'].nunique()\n",
    "\n",
    "# Count unique MovieIDs in the testing data\n",
    "unique_movies_testing = mapped_training_data['CustomerID'].nunique()\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Unique CustomerIDs in Training Data: {unique_users_training}\")\n",
    "print(f\"Unique MovieIDs in Training Data: {unique_movies_testing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of CSR Matrix Format\n",
    "\n",
    "In this dataset, utilizing the Compressed Sparse Row (CSR) matrix format offers significant advantages:\n",
    "\n",
    "1. **Memory Efficiency**: The CSR matrix format stores only the non-zero elements of the matrix, reducing memory usage substantially, especially in cases of sparse data where most entries are zero.\n",
    "\n",
    "2. **Computational Efficiency**: Operations involving CSR matrices, such as matrix multiplication and addition, are optimized for sparse matrices, resulting in faster computations compared to dense matrices.\n",
    "\n",
    "3. **Scalability**: With large datasets, using CSR matrices ensures scalability by efficiently handling sparse data structures without consuming excessive memory resources.\n",
    "\n",
    "By leveraging the CSR matrix format, the memory-efficient representation of user-item ratings, along with additional feature matrices, can be effectively managed and processed, enabling robust analysis and modeling of the dataset while mitigating memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define values for user features\n",
    "user_activity_values = mapped_training_data['user_activity'].values\n",
    "AverageMovieAgeRated_values = mapped_training_data['AverageMovieAgeRated'].values\n",
    "user_avg_rating_values = mapped_training_data['user_average_rating'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# Extract user indexes and movie indexes\n",
    "user_indexes = mapped_training_data['UserIndex'].values\n",
    "movie_indexes = mapped_training_data['MovieIndex'].values\n",
    "ratings = mapped_training_data['Rating'].values\n",
    "\n",
    "# Create the base user-item ratings CSR matrix\n",
    "num_users = user_indexes.max() + 1\n",
    "num_movies = movie_indexes.max() + 1\n",
    "ratings_csr_matrix = csr_matrix((ratings, (user_indexes, movie_indexes)), shape=(num_users, num_movies))\n",
    "\n",
    "# Create CSR matrices for features\n",
    "user_activity_matrix = csr_matrix((user_activity_values, (user_indexes, np.zeros_like(user_indexes))), shape=(num_users, 1))\n",
    "avg_movie_age_matrix = csr_matrix((AverageMovieAgeRated_values, (user_indexes, np.zeros_like(user_indexes))), shape=(num_users, 1))\n",
    "user_avg_rating_matrix = csr_matrix((user_avg_rating_values, (user_indexes, np.zeros_like(user_indexes))), shape=(num_users, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note \n",
    "I have created a full CSR matrix containing user features to mitigate sparsity and get better accuracy. but when calculating similarity matrix I enconter memory error so I will continue with simple rating CSR marix containing userid, movieid and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontally stack the features matrices with the ratings CSR matrix\n",
    "full_csr_matrix = hstack([ratings_csr_matrix, user_activity_matrix])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create similarity matrix \n",
    "### Use of Pre-Calculated Similarity Matrix\n",
    "\n",
    "In the process of developing the recommendation system, I initially attempted to compute similarities between users using a for loop. However, this approach proved to be inefficient and led to a significant decrease in recommendation process speed. To address this issue and improve efficiency, I opted to use a pre-calculated similarity matrix.\n",
    "\n",
    "#### Advantages of Pre-Calculated Similarity Matrix:\n",
    "1. **Enhanced Performance**: By pre-calculating the similarity matrix, the recommendation process becomes much faster compared to computing similarities on-the-fly during each recommendation request.\n",
    "\n",
    "2. **Optimized Resource Utilization**: Utilizing a pre-calculated similarity matrix optimizes resource utilization, as the computational overhead of similarity calculations is shifted to a one-time operation during the matrix creation phase.\n",
    "\n",
    "#### Considerations and Limitations:\n",
    "- **Memory Constraints**: While pre-calculating the similarity matrix addresses computational efficiency concerns, it may introduce memory constraints, especially with large datasets. Consequently, I had to be mindful of memory usage and may have skipped certain similarity methods, such as Manhattan distance, to avoid memory errors.\n",
    "\n",
    "- **Challenges with Pearson Correlation**: Initially, I explored Pearson correlation as a similarity metric. However, due to its requirement for at least two common ratings for each pair of users, it often resulted in NaN (Not a Number) values for most data points. This limitation arises from the sparse nature of the dataset, where many user pairs lack sufficient common ratings for accurate correlation calculation.\n",
    "\n",
    "- **Example of Missing Data Impact**: For instance, if two users rated only one movie in common, they might still share similar preferences. However, Pearson correlation cannot capture this similarity due to the lack of sufficient common ratings.\n",
    "\n",
    "- **Sparse Data Challenges with Cosine Similarity**: Even with cosine similarity, which is well-suited for sparse data, there may be instances where multiple user pairs exhibit exactly the same similarity scores. This redundancy can potentially introduce complications in the recommendation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "cosine_similarity_matrix_csr = cosine_similarity(ratings_csr_matrix, dense_output=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict ratings using similarities\n",
    "\n",
    "The `predict_rating` function is designed to estimate the rating a target user would give to a specific movie based on the ratings of similar users. Below is a brief overview of the prediction process:\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Identify Users Who Rated the Movie**:\n",
    "   - The function first identifies users who have rated the movie of interest. This is done by extracting non-zero entries from the corresponding column of the user-item matrix.\n",
    "\n",
    "2. **Extract Similarity Scores**:\n",
    "   - Next, it extracts the similarity scores between the target user and all other users from the pre-calculated similarity matrix.\n",
    "\n",
    "3. **Filter Similarities for Rated Users**:\n",
    "   - The similarity scores are filtered to retain only those corresponding to users who have rated the movie.\n",
    "\n",
    "4. **Select Top Similar Users**:\n",
    "   - Among the users who have rated the movie, the function selects the top-k similar users based on their similarity scores.\n",
    "\n",
    "5. **Retrieve Ratings of Top Similar Users**:\n",
    "   - It retrieves the ratings of the movie from the top-k similar users in the user-item matrix.\n",
    "\n",
    "6. **Calculate Weighted Average Rating**:\n",
    "   - Using the similarity scores as weights, the function calculates a weighted sum of the ratings provided by the top-k similar users.\n",
    "\n",
    "7. **Predict Rating**:\n",
    "   - Finally, it predicts the rating for the target user by dividing the weighted sum by the sum of similarity scores. If the sum of similarity scores is zero, indicating no similar users, it defaults to the overall average rating of the movie.\n",
    "\n",
    "#### Return:\n",
    "- The function returns the predicted rating for the movie by the target user.\n",
    "\n",
    "This prediction logic leverages the collaborative filtering approach, where recommendations are based on the preferences and behaviors of similar users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(csr_user_item_matrix, similarity_matrix, user_index, movie_index, k):\n",
    "    \"\"\"\n",
    "    Predict the rating for a given movie by a target user, based on the ratings of top-k similar users.\n",
    "    This function uses a pre-calculated similarity matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - csr_user_item_matrix: CSR matrix representing the user-item matrix.\n",
    "    - similarity_matrix: CSR matrix representing the similarity scores between users.\n",
    "    - user_index: The index of the user for whom the rating is being predicted.\n",
    "    - movie_index: The index of the movie for which the rating is being predicted.\n",
    "    - k: Number of top similar users to consider for prediction.\n",
    "    \n",
    "    Returns:\n",
    "    - Predicted rating for the movie by the target user.\n",
    "    \"\"\"\n",
    "    # Step 1: Identify users who have rated the movie\n",
    "    movie_rated_indices = csr_user_item_matrix[:, movie_index].nonzero()[0]\n",
    "    \n",
    "    # Check if user_index exists in the similarity matrix\n",
    "    if user_index < 0 or user_index >= similarity_matrix.shape[0]:\n",
    "        # If user_index does not exist in similarity matrix, return default prediction\n",
    "        overall_average_rating = csr_user_item_matrix[:, movie_index].data.mean()\n",
    "        return overall_average_rating if np.isfinite(overall_average_rating) else 3.0  # Assuming 3.0 as a neutral rating\n",
    "    \n",
    "    # Step 2: Extract similarity scores for the target user with all other users\n",
    "    user_similarities = similarity_matrix.getrow(user_index).toarray().flatten()\n",
    "    \n",
    "    # Step 3: Filter the similarities for users who have rated the movie\n",
    "    filtered_similarities = user_similarities[movie_rated_indices]\n",
    "    \n",
    "    # Step 4: Get indices of top k similar users among those who have rated the movie\n",
    "    top_k_indices = np.argsort(filtered_similarities)[-k:]\n",
    "    top_k_users_indices = movie_rated_indices[top_k_indices]\n",
    "    top_k_similarities = filtered_similarities[top_k_indices]\n",
    "\n",
    "    # Retrieve ratings for the movie from these top-k similar users\n",
    "    top_k_ratings = csr_user_item_matrix[top_k_users_indices, movie_index].toarray().flatten()\n",
    "\n",
    "    # Calculate the weighted average rating\n",
    "    weighted_sum = np.dot(top_k_similarities, top_k_ratings)\n",
    "    similarity_sum = np.sum(top_k_similarities)\n",
    "    \n",
    "    if similarity_sum > 0:\n",
    "        predicted_rating = weighted_sum / similarity_sum\n",
    "    else:\n",
    "        # Use the overall average rating of the movie by all users as the default rating\n",
    "        overall_average_rating = csr_user_item_matrix[:, movie_index].data.mean()\n",
    "        predicted_rating = overall_average_rating if np.isfinite(overall_average_rating) else 3.0  # Assuming 3.0 as a neutral rating\n",
    "\n",
    "    return predicted_rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Predictions\n",
    "\n",
    "The `evaluate_predictions_csr` function assesses the performance of the recommendation system by predicting ratings for each user-movie pair in the validation set using a CSR matrix and a pre-computed similarity matrix and previous defined prediction function. The predictions are then compared to the actual ratings, and the root mean square error (RMSE) is calculated as a measure of prediction accuracy. Here's an overview of the evaluation process:\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Iterate Over Validation Data**:\n",
    "   - The function iterates over each row in the validation data, which typically contains 'UserIndex', 'MovieIndex', and 'Rating'.\n",
    "\n",
    "2. **Predict Ratings**:\n",
    "   - For each user-movie pair in the validation set, the function predicts the rating using the `predict_rating` function. This prediction is based on the pre-calculated CSR matrix representing the user-item matrix from the training set and the similarity matrix.\n",
    "\n",
    "3. **Calculate Actual Ratings**:\n",
    "   - The actual ratings from the validation data are stored for comparison.\n",
    "\n",
    "4. **Compute RMSE**:\n",
    "   - After obtaining both the actual and predicted ratings, the function calculates the root mean square error (RMSE) between them. RMSE is a commonly used metric to measure the average deviation of predicted ratings from the actual ratings.\n",
    "\n",
    "#### Return:\n",
    "- The function returns the RMSE value, representing the degree of error in the prediction of ratings compared to the ground truth ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def evaluate_predictions_csr(validation_data, csr_user_item_matrix, similarity_matrix, k):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendation system by predicting ratings for each user-movie pair in the validation set\n",
    "    using a CSR matrix and pre-computed similarity matrix, and comparing the predictions to the actual ratings using RMSE.\n",
    "\n",
    "    Parameters:\n",
    "    - validation_data: DataFrame containing 'UserIndex', 'MovieIndex', and 'Rating'.\n",
    "    - csr_user_item_matrix: CSR matrix representing the user-item matrix from the training set.\n",
    "    - similarity_matrix: Pre-computed similarity matrix as a CSR matrix.\n",
    "    - k: The number of top similar users to consider when making predictions.\n",
    "    \n",
    "    Returns:\n",
    "    - rmse: The root mean square error of the predicted ratings against the actual ratings.\n",
    "    \"\"\"\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for _, row in validation_data.iterrows():\n",
    "        user_index = row['UserIndex']\n",
    "        movie_index = row['MovieIndex']\n",
    "        actual_rating = row['Rating']\n",
    "        \n",
    "        # Check if the movie index is valid\n",
    "        if movie_index in csr_user_item_matrix.indices:\n",
    "            predicted_rating = predict_rating(csr_user_item_matrix, similarity_matrix, user_index, movie_index, k)\n",
    "            actual_ratings.append(actual_rating)\n",
    "            predicted_ratings.append(predicted_rating)\n",
    "    \n",
    "    # Calculate RMSE between actual and predicted ratings\n",
    "    actual_ratings = np.array(actual_ratings)\n",
    "    predicted_ratings = np.array(predicted_ratings)\n",
    "    valid_mask = ~np.isnan(predicted_ratings)\n",
    "    rmse = sqrt(mean_squared_error(actual_ratings[valid_mask], predicted_ratings[valid_mask]))\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation for Parameter Tuning\n",
    "\n",
    "At this block we performs k-fold cross-validation to tune the hyperparameter 'k' for the K-Nearest Neighbors (KNN) recommendation system. \n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Setup KFold Cross-Validation**:\n",
    "   - Utilizes the `KFold` class from scikit-learn to create a k-fold cross-validation iterator. The data is split into 5 folds (`n_splits=5`) with shuffling enabled for randomization.\n",
    "\n",
    "2. **Define Hyperparameters**:\n",
    "   - Specifies a list of k values (`k_values`) to test. These represent the number of top similar users considered for prediction.\n",
    "\n",
    "3. **Initialize Results Storage**:\n",
    "   - Creates an empty list (`results`) to store the average RMSE for each k value over all folds.\n",
    "\n",
    "4. **Cross-Validation Loop**:\n",
    "   - Iterates over each k value.\n",
    "   - Within each iteration, iterates over each fold generated by the KFold iterator.\n",
    "   - For each fold, evaluates the predictions using the `evaluate_predictions_csr` function, passing the validation data, CSR matrix of user-item ratings, similarity matrix, and the current k value.\n",
    "   - Computes the RMSE for each fold and stores it in `fold_rmses`.\n",
    "\n",
    "5. **Average RMSE Calculation**:\n",
    "   - Calculates the average RMSE for the current k value by taking the mean of all fold RMSEs.\n",
    "\n",
    "6. **Results Storage**:\n",
    "   - Appends a tuple `(k, avg_rmse)` to the `results` list, containing the k value and its corresponding average RMSE.\n",
    "\n",
    "7. **Identify Best k Value**:\n",
    "   - Finds the best k value based on the minimum average RMSE from the results list.\n",
    "   - Prints the best k value along with its corresponding RMSE.\n",
    "\n",
    "#### Return:\n",
    "- The best k value along with its RMSE, providing insight into the optimal choice of hyperparameter for the KNN recommendation system.\n",
    "\n",
    "This cross-validation procedure helps in selecting the most suitable k value that minimizes prediction errors and improves the overall performance of the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5, Average RMSE=0.9508280757776708\n",
      "k=20, Average RMSE=0.9585441076013355\n",
      "k=30, Average RMSE=0.9618954676997873\n",
      "k=50, Average RMSE=0.964437779735535\n",
      "k=100, Average RMSE=0.9668930415522464\n",
      "k=200, Average RMSE=0.9678785252100521\n",
      "Best k: 5 with RMSE: 0.9508280757776708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming k_values to test and your similarity matrix is already defined\n",
    "k_values = [5, 20, 30, 50, 100, 200]\n",
    "similarity_matrix = cosine_similarity_matrix_csr\n",
    "# Setup KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    fold_rmses = []  # Store RMSEs for each fold\n",
    "\n",
    "    for train_indices, test_indices in kf.split(ratings_csr_matrix):\n",
    "        # Splitting your data: ratings_csr_matrix doesn't change, so you just map validation set indices\n",
    "        validation_data_fold = mapped_training_data.iloc[test_indices]\n",
    "\n",
    "        # Evaluate predictions on this fold's test set\n",
    "        rmse = evaluate_predictions_csr(validation_data_fold, ratings_csr_matrix, similarity_matrix, k)\n",
    "        fold_rmses.append(rmse)\n",
    "\n",
    "    # Calculate average RMSE for this k over all folds\n",
    "    avg_rmse = np.mean(fold_rmses)\n",
    "    results.append((k, avg_rmse))\n",
    "    print(f\"k={k}, Average RMSE={avg_rmse}\")\n",
    "\n",
    "# Find the best k value based on average RMSE\n",
    "best_k, best_rmse = min(results, key=lambda x: x[1])\n",
    "print(f\"Best k: {best_k} with RMSE: {best_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map testing data to their indecies\n",
    "mapped_testing_data = map_ids_to_indices(testing_data, user_id_to_index, movie_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0653943639743557\n"
     ]
    }
   ],
   "source": [
    "# calculate rmse with the best k on test set\n",
    "similarity_matrix = cosine_similarity_matrix_csr\n",
    "k = 5  # Example value for k\n",
    "rmse = evaluate_predictions_csr(mapped_testing_data, ratings_csr_matrix, similarity_matrix, k)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Recommendation Based on Predicted Ratings\n",
    "\n",
    "Function `recommend_movies_prediction` generates movie recommendations for specified user(s) by predicting ratings and suggesting the top n movies with the highest predicted ratings. Here's a breakdown of its functionality:\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Input Validation**:\n",
    "   - Checks if the input `user_ids` is a list. If not, converts it to a list to handle both single and multiple user recommendations.\n",
    "\n",
    "2. **Iterate Over User IDs**:\n",
    "   - Loops through each user ID provided in the input list.\n",
    "\n",
    "3. **User Index Retrieval**:\n",
    "   - Retrieves the user index from the DataFrame (`df`) corresponding to the given user ID.\n",
    "\n",
    "4. **Unrated Movies Detection**:\n",
    "   - Finds the indices of movies that the user has not rated yet. This is achieved by comparing all movie indices against the nonzero indices of the user's row in the user-item matrix (`csr_user_item_matrix`).\n",
    "\n",
    "5. **Predict Ratings for Unrated Movies**:\n",
    "   - Iterates over the unrated movies' indices.\n",
    "   - For each unrated movie, predicts the rating using the `predict_rating` function, passing the user index, movie index, K value, and similarity matrix.\n",
    "\n",
    "6. **Top n Recommendations**:\n",
    "   - Sorts the predicted ratings in descending order and selects the top n movies.\n",
    "   - Constructs a list of tuples containing movie IDs and their corresponding predicted ratings.\n",
    "\n",
    "7. **Return Recommendations**:\n",
    "   - Constructs a dictionary where keys are user IDs and values are lists of top n movie recommendations with their predicted ratings.\n",
    "\n",
    "#### Return:\n",
    "- A dictionary containing user IDs as keys and lists of top n recommended movies with their predicted ratings as values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recommend_movies_prediction(df, csr_user_item_matrix, similarity_matrix, user_ids, k, n):\n",
    "    \"\"\"\n",
    "    Recommend top n movies for specified user(s) based on predicted ratings.\n",
    "    Assumes 'UserIndex' and 'MovieIndex' are available in 'df'.\n",
    "    \"\"\"\n",
    "    if not isinstance(user_ids, list):\n",
    "        user_ids = [user_ids]\n",
    "\n",
    "    recommendations = {}\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        try:\n",
    "            user_index = df[df['CustomerID'] == user_id]['UserIndex'].iloc[0]  # Assuming first matching UserIndex is representative\n",
    "        except IndexError:\n",
    "            print(f\"User ID {user_id} not found.\")\n",
    "            continue\n",
    "\n",
    "        unrated_movies_indices = np.setdiff1d(np.arange(csr_user_item_matrix.shape[1]),\n",
    "                                               csr_user_item_matrix.getrow(user_index).nonzero()[1])\n",
    "        \n",
    "        predicted_ratings = []\n",
    "        for movie_index in unrated_movies_indices:\n",
    "            predicted_rating = predict_rating(csr_user_item_matrix, similarity_matrix, user_index, movie_index, k)\n",
    "            movie_id = {v: k for k, v in movie_id_to_index.items()}[movie_index]  # Reverse lookup to get MovieID from MovieIndex\n",
    "            predicted_ratings.append((movie_id, predicted_rating))\n",
    "\n",
    "        top_n_recommendations = sorted(predicted_ratings, key=lambda x: x[1], reverse=True)[:n]\n",
    "        recommendations[user_id] = top_n_recommendations\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare main data for movie recommendation\n",
    "main_mapped_data = map_ids_to_indices(strat_sample_df, user_id_to_index, movie_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the rows (user indices), columns (movie indices), and data (ratings) for the CSR matrix\n",
    "rows = main_mapped_data['UserIndex'].values\n",
    "cols = main_mapped_data['MovieIndex'].values\n",
    "data = main_mapped_data['Rating'].values\n",
    "\n",
    "# Determine the shape of the CSR matrix\n",
    "# The shape is (max_user_index + 1, max_movie_index + 1) because indices start from 0\n",
    "num_users = main_mapped_data['UserIndex'].max() + 1\n",
    "num_movies = main_mapped_data['MovieIndex'].max() + 1\n",
    "\n",
    "# Create the main CSR matrix\n",
    "Main_csr_matrix = csr_matrix((data, (rows, cols)), shape=(num_users, num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the main similarity matrix\n",
    "main_similarity_matrix = cosine_similarity(Main_csr_matrix, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>YearOfRelease</th>\n",
       "      <th>MovieTitle</th>\n",
       "      <th>RatingYear</th>\n",
       "      <th>user_activity</th>\n",
       "      <th>AverageMovieAgeRated</th>\n",
       "      <th>user_average_rating</th>\n",
       "      <th>scaled_movie_age</th>\n",
       "      <th>UserIndex</th>\n",
       "      <th>MovieIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8464880</th>\n",
       "      <td>1693</td>\n",
       "      <td>1851346</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-12-15</td>\n",
       "      <td>1998</td>\n",
       "      <td>Sphere</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.301075</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311316</th>\n",
       "      <td>1220</td>\n",
       "      <td>1710563</td>\n",
       "      <td>4</td>\n",
       "      <td>2004-12-03</td>\n",
       "      <td>2004</td>\n",
       "      <td>Man on Fire</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.129032</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17205447</th>\n",
       "      <td>3316</td>\n",
       "      <td>17864</td>\n",
       "      <td>3</td>\n",
       "      <td>2004-01-07</td>\n",
       "      <td>2002</td>\n",
       "      <td>Bartleby</td>\n",
       "      <td>2004</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.215054</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22300142</th>\n",
       "      <td>4227</td>\n",
       "      <td>1673744</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-02-26</td>\n",
       "      <td>1997</td>\n",
       "      <td>The Full Monty</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.258065</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146126</th>\n",
       "      <td>1202</td>\n",
       "      <td>1321440</td>\n",
       "      <td>4</td>\n",
       "      <td>2004-06-25</td>\n",
       "      <td>1983</td>\n",
       "      <td>National Lampoon's Vacation</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.032258</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MovieID CustomerID  Rating        Date  YearOfRelease  \\\n",
       "8464880     1693    1851346       1  2002-12-15           1998   \n",
       "6311316     1220    1710563       4  2004-12-03           2004   \n",
       "17205447    3316      17864       3  2004-01-07           2002   \n",
       "22300142    4227    1673744       4  2000-02-26           1997   \n",
       "6146126     1202    1321440       4  2004-06-25           1983   \n",
       "\n",
       "                           MovieTitle  RatingYear  user_activity  \\\n",
       "8464880                        Sphere        2002              1   \n",
       "6311316                   Man on Fire        2004              1   \n",
       "17205447                     Bartleby        2004              2   \n",
       "22300142               The Full Monty        2000              1   \n",
       "6146126   National Lampoon's Vacation        2004              1   \n",
       "\n",
       "          AverageMovieAgeRated  user_average_rating  scaled_movie_age  \\\n",
       "8464880                      2                    3          1.301075   \n",
       "6311316                      2                    3          1.129032   \n",
       "17205447                     1                    3          1.215054   \n",
       "22300142                     2                    4          1.258065   \n",
       "6146126                      2                    4          2.032258   \n",
       "\n",
       "          UserIndex  MovieIndex  \n",
       "8464880           0           0  \n",
       "6311316           1           1  \n",
       "17205447          2           2  \n",
       "22300142          3           3  \n",
       "6146126           4           4  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_mapped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for User ID 1851346:\n",
      "\tMovie ID: 30, Predicted Rating: 5.000000000000001\n",
      "\tMovie ID: 3122, Predicted Rating: 5.000000000000001\n",
      "\tMovie ID: 1495, Predicted Rating: 5.000000000000001\n",
      "\tMovie ID: 2780, Predicted Rating: 5.0\n",
      "\tMovie ID: 2342, Predicted Rating: 5.0\n"
     ]
    }
   ],
   "source": [
    "# testing recommendation function with a specific user\n",
    "user_ids = ['1851346']  # Single user example\n",
    "k = 5  # Number of similar users to consider\n",
    "n = 5  # Number of recommendations to generate\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = recommend_movies_prediction(main_mapped_data, Main_csr_matrix, main_similarity_matrix, user_ids, k, n)\n",
    "\n",
    "# Display the recommendations\n",
    "for user_id in user_ids:\n",
    "    print(f\"Recommendations for User ID {user_id}:\")\n",
    "    if user_id in recommendations:\n",
    "        for movie_id, predicted_rating in recommendations[user_id]:\n",
    "            print(f\"\\tMovie ID: {movie_id}, Predicted Rating: {predicted_rating}\")\n",
    "    else:\n",
    "        print(\"\\tNo recommendations available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for User ID 1851346 and Movie ID 886 is: 2.0 \n"
     ]
    }
   ],
   "source": [
    "# Testing prediction function with one user to make sure system predicts all range of ratings (not just 5)\n",
    "user_index = 2  # Replace with actual user ID\n",
    "movie_index = 10  # Replace with actual movie ID\n",
    "K = 200  # Number of neighbors\n",
    "predicted_rating = predict_rating(Main_csr_matrix, main_similarity_matrix, user_index, movie_index, k)\n",
    "print(f\"Predicted rating for User ID {user_id} and Movie ID {movie_id} is: {predicted_rating} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Recommendation Based on Classification (Voting) Logic\n",
    "\n",
    "In this section, we employ a classification approach to predict movie ratings for a target user. Unlike the previous method, which focused on predicting numerical ratings, the classification method treats ratings as categories (1 to 5 stars) and utilizes voting logic to determine the predicted rating. Here's a summary of the classification method:\n",
    "\n",
    "#### Prediction Logic:\n",
    "\n",
    "1. **Identify Users Who Rated the Movie**:\n",
    "   - Determine the indices of users who have rated the target movie in the user-item matrix.\n",
    "\n",
    "2. **Extract Similarity Scores**:\n",
    "   - Retrieve the similarity scores between the target user and all other users from the pre-calculated similarity matrix.\n",
    "\n",
    "3. **Filter Similarities for Rated Users**:\n",
    "   - Filter the similarity scores to include only those users who have rated the movie of interest.\n",
    "\n",
    "4. **Select Top K Similar Users**:\n",
    "   - Identify the top K similar users among those who have rated the movie, based on their similarity scores.\n",
    "\n",
    "5. **Retrieve Ratings and Calculate Votes**:\n",
    "   - Retrieve the ratings of the movie from the top K similar users.\n",
    "   - Count the votes for each rating category (1 to 5 stars), weighted by the similarity scores.\n",
    "\n",
    "6. **Determine Predicted Rating**:\n",
    "   - The predicted rating is determined by selecting the rating category with the highest sum of similarity-weighted votes.\n",
    "\n",
    "7. **Handling Default Rating**:\n",
    "   - If the predicted rating is not available (e.g., due to insufficient data), the overall average rating of the movie (nonzero) by all users is used as the default prediction.\n",
    "\n",
    "#### Evaluation and Performance:\n",
    "\n",
    "- The classification method results in predictions based on a voting scheme, where similar users contribute to the final prediction through their weighted votes.\n",
    "- The Root Mean Square Error (RMSE) is used to evaluate the performance of the classification-based recommendation system.\n",
    "- During evaluation, the best-performing value of K (number of top similar users) is determined empirically. In this case, the best K value was found to be 200, resulting in the lowest RMSE among tested values.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "- The classification method offers an alternative approach to predicting movie ratings, particularly suitable for scenarios where numerical predictions are challenging or less effective.\n",
    "- Comparing to prediction method, classification method resulted in higher rmse with k=200\n",
    "\n",
    "### Note\n",
    "Since I have used the same functions and methods as before, I skip explaining each function again to respect the time of the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def predict_rating_with_classification(csr_user_item_matrix, similarity_matrix, user_index, movie_index, k):\n",
    "    \"\"\"\n",
    "    Predict the rating for a given movie by a target user, based on the ratings of top-k similar users using classification (voting) logic.\n",
    "    This function uses a pre-calculated similarity matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - csr_user_item_matrix: CSR matrix representing the user-item matrix.\n",
    "    - similarity_matrix: CSR matrix representing the similarity scores between users.\n",
    "    - user_index: The index of the user for whom the rating is being predicted.\n",
    "    - movie_index: The index of the movie for which the rating is being predicted.\n",
    "    - k: Number of top similar users to consider for prediction.\n",
    "\n",
    "    Returns:\n",
    "    - Predicted rating for the movie by the target user.\n",
    "    \"\"\"\n",
    "    # Step 1: Identify users who have rated the movie\n",
    "    movie_rated_indices = csr_user_item_matrix[:, movie_index].nonzero()[0]\n",
    "    \n",
    "    # Check if user_index exists in the similarity matrix\n",
    "    if user_index < 0 or user_index >= similarity_matrix.shape[0]:\n",
    "        # If user_index does not exist in similarity matrix, return default prediction\n",
    "        non_zero_ratings = csr_user_item_matrix[:, movie_index][csr_user_item_matrix[movie_index] != 0]\n",
    "        predicted_rating = non_zero_ratings.mean() if len(non_zero_ratings) > 0 else np.nan # calculate average just concidering non zero ratings \n",
    "  \n",
    "    # Step 2: Extract similarity scores for the target user with all other users\n",
    "    user_similarities = similarity_matrix.getrow(user_index).toarray().flatten()\n",
    "    \n",
    "    # Step 3: Filter the similarities for users who have rated the movie\n",
    "    filtered_similarities = user_similarities[movie_rated_indices]\n",
    "    \n",
    "    # Step 4: Get indices of top k similar users among those who have rated the movie\n",
    "    top_k_indices = np.argsort(filtered_similarities)[-k:]\n",
    "    top_k_users_indices = movie_rated_indices[top_k_indices]\n",
    "    top_k_similarities = filtered_similarities[top_k_indices]\n",
    "\n",
    "    # Retrieve ratings for the movie from these top-k similar users\n",
    "    top_k_ratings = csr_user_item_matrix[top_k_users_indices, movie_index].toarray().flatten()\n",
    "    \n",
    "    # Create a dictionary to hold the count of votes for each rating\n",
    "    rating_votes = defaultdict(int)\n",
    "    \n",
    "   # Calculate weights based on similarities and count votes for each rating\n",
    "    for similarity, rating in zip(top_k_similarities, top_k_ratings):\n",
    "        if rating in [1, 2, 3, 4, 5]:\n",
    "            rating_votes[rating] += similarity\n",
    "\n",
    "    # Find the rating with the highest sum of similarity weights\n",
    "    predicted_rating = max(rating_votes, key=rating_votes.get, default=np.nan)\n",
    "    \n",
    "    # Use the overall average rating of the movie by all users as the default rating\n",
    "    if np.isnan(predicted_rating):\n",
    "        non_zero_ratings = csr_user_item_matrix[:, movie_id][csr_user_item_matrix[movie_id] != 0]\n",
    "        predicted_rating = non_zero_ratings.mean() if len(non_zero_ratings) > 0 else np.nan # calculate average just concidering non zero ratings \n",
    "\n",
    "    return predicted_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def evaluate_predictions_classification(validation_data, csr_user_item_matrix, similarity_matrix, k):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendation system by predicting ratings for each user-movie pair in the validation set\n",
    "    using a CSR matrix and pre-computed similarity matrix, and comparing the predictions to the actual ratings using RMSE.\n",
    "\n",
    "    Parameters:\n",
    "    - validation_data: DataFrame containing 'UserIndex', 'MovieIndex', and 'Rating'.\n",
    "    - csr_user_item_matrix: CSR matrix representing the user-item matrix from the training set.\n",
    "    - similarity_matrix: Pre-computed similarity matrix as a CSR matrix.\n",
    "    - k: The number of top similar users to consider when making predictions.\n",
    "    \n",
    "    Returns:\n",
    "    - rmse: The root mean square error of the predicted ratings against the actual ratings.\n",
    "    \"\"\"\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for _, row in validation_data.iterrows():\n",
    "        user_index = row['UserIndex']\n",
    "        movie_index = row['MovieIndex']\n",
    "        actual_rating = row['Rating']\n",
    "        \n",
    "        # Check if the movie index is valid\n",
    "        if movie_index in csr_user_item_matrix.indices:\n",
    "            predicted_rating = predict_rating_with_classification(csr_user_item_matrix, similarity_matrix, user_index, movie_index, k)\n",
    "            actual_ratings.append(actual_rating)\n",
    "            predicted_ratings.append(predicted_rating)\n",
    "    \n",
    "    # Calculate RMSE between actual and predicted ratings\n",
    "    actual_ratings = np.array(actual_ratings)\n",
    "    predicted_ratings = np.array(predicted_ratings)\n",
    "    valid_mask = ~np.isnan(predicted_ratings)\n",
    "    rmse = sqrt(mean_squared_error(actual_ratings[valid_mask], predicted_ratings[valid_mask]))\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5, Average RMSE=1.1067832654649838\n",
      "k=20, Average RMSE=1.1092461534740492\n",
      "k=30, Average RMSE=1.1190036568259107\n",
      "k=50, Average RMSE=1.1130927111297735\n",
      "k=100, Average RMSE=1.0981874925760953\n",
      "k=200, Average RMSE=1.0940047558440196\n",
      "Best k: 200 with RMSE: 1.0940047558440196\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming k_values to test and your similarity matrix is already defined\n",
    "k_values = [5, 20, 30, 50, 100, 200]\n",
    "similarity_matrix = cosine_similarity_matrix_csr\n",
    "# Setup KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    fold_rmses = []  # Store RMSEs for each fold\n",
    "\n",
    "    for train_indices, test_indices in kf.split(ratings_csr_matrix):\n",
    "        # Splitting your data: ratings_csr_matrix doesn't change, so you just map validation set indices\n",
    "        validation_data_fold = mapped_training_data.iloc[test_indices]\n",
    "\n",
    "        # Evaluate predictions on this fold's test set\n",
    "        rmse = evaluate_predictions_classification(validation_data_fold, ratings_csr_matrix, similarity_matrix, k)\n",
    "        fold_rmses.append(rmse)\n",
    "\n",
    "    # Calculate average RMSE for this k over all folds\n",
    "    avg_rmse = np.mean(fold_rmses)\n",
    "    results.append((k, avg_rmse))\n",
    "    print(f\"k={k}, Average RMSE={avg_rmse}\")\n",
    "\n",
    "# Find the best k value based on average RMSE\n",
    "best_k, best_rmse = min(results, key=lambda x: x[1])\n",
    "print(f\"Best k: {best_k} with RMSE: {best_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4861601426380566\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "similarity_matrix = cosine_similarity_matrix_csr\n",
    "k = 200  # Example value for k\n",
    "rmse = evaluate_predictions_classification(mapped_testing_data, ratings_csr_matrix, similarity_matrix, k)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recommend_movies_classification(df, csr_user_item_matrix, similarity_matrix, user_ids, k, n):\n",
    "    \"\"\"\n",
    "    Recommend top n movies for specified user(s) based on predicted ratings.\n",
    "    Assumes 'UserIndex' and 'MovieIndex' are available in 'df'.\n",
    "    \"\"\"\n",
    "    if not isinstance(user_ids, list):\n",
    "        user_ids = [user_ids]\n",
    "\n",
    "    recommendations = {}\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        try:\n",
    "            user_index = df[df['CustomerID'] == user_id]['UserIndex'].iloc[0]  # Assuming first matching UserIndex is representative\n",
    "        except IndexError:\n",
    "            print(f\"User ID {user_id} not found.\")\n",
    "            continue\n",
    "\n",
    "        unrated_movies_indices = np.setdiff1d(np.arange(csr_user_item_matrix.shape[1]),\n",
    "                                               csr_user_item_matrix.getrow(user_index).nonzero()[1])\n",
    "        \n",
    "        predicted_ratings = []\n",
    "        for movie_index in unrated_movies_indices:\n",
    "            predicted_rating = predict_rating_with_classification(csr_user_item_matrix, similarity_matrix, user_index, movie_index, k)\n",
    "            movie_id = {v: k for k, v in movie_id_to_index.items()}[movie_index]  # Reverse lookup to get MovieID from MovieIndex\n",
    "            predicted_ratings.append((movie_id, predicted_rating))\n",
    "\n",
    "        top_n_recommendations = sorted(predicted_ratings, key=lambda x: x[1], reverse=True)[:n]\n",
    "        recommendations[user_id] = top_n_recommendations\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for User ID 1851346:\n",
      "\tMovie ID: 2300, Predicted Rating: 5\n",
      "\tMovie ID: 1783, Predicted Rating: 5\n",
      "\tMovie ID: 2780, Predicted Rating: 5\n",
      "\tMovie ID: 3416, Predicted Rating: 5\n",
      "\tMovie ID: 886, Predicted Rating: 5\n"
     ]
    }
   ],
   "source": [
    "# testing recommendation function with one user \n",
    "user_ids = ['1851346']  # Single user example\n",
    "k = 200  # Number of similar users to consider\n",
    "n = 5  # Number of recommendations to generate\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = recommend_movies_classification(main_mapped_data, Main_csr_matrix, main_similarity_matrix, user_ids, k, n)\n",
    "\n",
    "# Display the recommendations\n",
    "for user_id in user_ids:\n",
    "    print(f\"Recommendations for User ID {user_id}:\")\n",
    "    if user_id in recommendations:\n",
    "        for movie_id, predicted_rating in recommendations[user_id]:\n",
    "            print(f\"\\tMovie ID: {movie_id}, Predicted Rating: {predicted_rating}\")\n",
    "    else:\n",
    "        print(\"\\tNo recommendations available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for User ID 1851346 and Movie ID 886 is: 3 \n"
     ]
    }
   ],
   "source": [
    "# Testing prediction function with one user to make sure system predicts all range of ratings (not just 5)\n",
    "user_index = 1  # Replace with actual user ID\n",
    "movie_index = 10  # Replace with actual movie ID\n",
    "K = 200  # Number of neighbors\n",
    "predicted_rating = predict_rating_with_classification (Main_csr_matrix, main_similarity_matrix, user_index, movie_index, k)\n",
    "print(f\"Predicted rating for User ID {user_id} and Movie ID {movie_id} is: {predicted_rating} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
