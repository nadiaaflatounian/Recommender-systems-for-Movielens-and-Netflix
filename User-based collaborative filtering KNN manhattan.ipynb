{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>YearOfRelease</th>\n",
       "      <th>RatingYear</th>\n",
       "      <th>MovieAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1488844</td>\n",
       "      <td>3</td>\n",
       "      <td>2005-09-06</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>822109</td>\n",
       "      <td>5</td>\n",
       "      <td>2005-05-13</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>885013</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-10-19</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30878</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-12-26</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>823519</td>\n",
       "      <td>3</td>\n",
       "      <td>2004-05-03</td>\n",
       "      <td>2003</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID  CustomerID  Rating        Date  YearOfRelease  RatingYear  \\\n",
       "0        1     1488844       3  2005-09-06           2003        2005   \n",
       "1        1      822109       5  2005-05-13           2003        2005   \n",
       "2        1      885013       4  2005-10-19           2003        2005   \n",
       "3        1       30878       4  2005-12-26           2003        2005   \n",
       "4        1      823519       3  2004-05-03           2003        2004   \n",
       "\n",
       "   MovieAge  \n",
       "0         2  \n",
       "1         2  \n",
       "2         2  \n",
       "3         2  \n",
       "4         1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "training_df = pd.read_csv('C:/Users/nafla/OneDrive/Documents/system development/Netflix/training_data.csv')\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert CustomerID and MovieID to categorical types for memory efficiency\n",
    "# training_df['CustomerID'] = training_df['CustomerID'].astype(\"category\")\n",
    "# training_df['MovieID'] = training_df['MovieID'].astype(\"category\")\n",
    "# training_df['Rating'] = training_df['Rating'].astype('float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['CustomerID'] = training_df['CustomerID'].astype(str)\n",
    "training_df['MovieID'] = training_df['MovieID'].astype(str)\n",
    "training_df['Rating'] = pd.to_numeric(training_df['Rating'], errors='coerce')  # Converts to float, makes non-numeric as NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25     8.0\n",
      "0.50    24.0\n",
      "0.75    64.0\n",
      "Name: CustomerID, dtype: float64\n",
      "0.25     192.0\n",
      "0.50     552.5\n",
      "0.75    2539.0\n",
      "Name: MovieID, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate quantiles for user activity and item popularity\n",
    "user_activity_quantiles = training_df['CustomerID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "item_popularity_quantiles = training_df['MovieID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "print(user_activity_quantiles)\n",
    "print(item_popularity_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each user and item to a bin based on the quantiles\n",
    "training_df['UserActivityBin'] = pd.qcut(training_df.groupby('CustomerID')['Rating'].transform('size'), \n",
    "                                q=[0, .25, .5, .75, 1], labels=['low', 'medium', 'medium-high', 'high'])\n",
    "\n",
    "training_df['ItemPopularityBin'] = pd.qcut(training_df.groupby('MovieID')['Rating'].transform('size'), \n",
    "                                  q=[0, .25, .5, .75, 1], labels=['low', 'medium', 'medium-high', 'high'])\n",
    "\n",
    "# Combine these with Rating to create a stratification key\n",
    "training_df['Strata'] = training_df['UserActivityBin'].astype(str) + \"_\" + training_df['ItemPopularityBin'].astype(str) + \"_\" + training_df['Rating'].astype(str)\n",
    "\n",
    "# Perform stratified sampling\n",
    "# we use groupby and  frac to specify a fraction of each strata and in case number of rows is less that 10 it takes all rows\n",
    "strat_sample_df = training_df.groupby('Strata').apply(lambda x: x.sample(frac=0.001 if len(x) > 10 else len(x)/len(x))).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the sampled DataFrame: 24052\n"
     ]
    }
   ],
   "source": [
    "num_sampled_rows = len(strat_sample_df)\n",
    "print(f\"Number of rows in the sampled DataFrame: {num_sampled_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "training_data, testing_data = train_test_split(strat_sample_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# further split the training data into training and validation sets\n",
    "training_data, validation_data = train_test_split(training_data, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user and item indxs\n",
    "num_users = training_data['CustomerID'].nunique()\n",
    "num_items = training_data['MovieID'].nunique()\n",
    "user_to_index = {user: idx for idx, user in enumerate(training_data['CustomerID'].unique())}\n",
    "item_to_index = {item: idx for idx, item in enumerate(training_data['MovieID'].unique())}\n",
    "\n",
    "# Defining user and item ids in training data\n",
    "user_ids = training_data['CustomerID'].unique()\n",
    "item_ids = training_data['MovieID'].unique()\n",
    "\n",
    "# Initialize a dense matrix\n",
    "train_matrix = np.zeros((num_users, num_items))\n",
    "\n",
    "# Not rated items are implicitly handled by the initial setup of the dense matrix using np.zeros\n",
    "# Populate the matrix with ratings\n",
    "for _, row in training_data.iterrows():\n",
    "    user_idx = user_to_index[row['CustomerID']]\n",
    "    item_idx = item_to_index[row['MovieID']]\n",
    "    train_matrix[user_idx, item_idx] = row['Rating']\n",
    "\n",
    "# Rated Items: Have their actual rating values as specified in training_data.\n",
    "# Not Rated Items: Are represented by a 0, indicating either the absence of a rating or implicitly treating unrated items as having a rating of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_ratings(user1_idx, user2_idx, user_item_matrix):\n",
    "    \"\"\"\n",
    "    Extracts dense rating vectors for common items rated by both users.\n",
    "    \"\"\"\n",
    "    user1_ratings = user_item_matrix[user1_idx, :]\n",
    "    user2_ratings = user_item_matrix[user2_idx, :]\n",
    "    \n",
    "    common_item_indices = np.where((user1_ratings > 0) & (user2_ratings > 0))[0]\n",
    "    \n",
    "    return user1_ratings[common_item_indices], user2_ratings[common_item_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "def manhattan_similarity(user1_common_ratings, user2_common_ratings):\n",
    "    \"\"\"\n",
    "    Computes the inverse Manhattan distance between two users' ratings as similarity.\n",
    "    \n",
    "    Args:\n",
    "    - user1_common_ratings, user2_common_ratings: Dense rating vectors for common items.\n",
    "    \n",
    "    Returns:\n",
    "    - The computed similarity score.\n",
    "    \"\"\"\n",
    "    if len(user1_common_ratings) == 0:  # Return 0 if there are no common ratings\n",
    "        return 0\n",
    "    # Calculate the Manhattan distance\n",
    "    distance = cityblock(user1_common_ratings, user2_common_ratings)\n",
    "    # Convert distance to similarity; add 1 to avoid division by zero\n",
    "    similarity = 1 / (1 + distance)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_idx, item_idx, k, user_item_matrix, user_ids):\n",
    "    \"\"\"\n",
    "    Predicts the rating for a specific user and item using KNN based on Manhattan similarity.\n",
    "\n",
    "    Args:\n",
    "    - user_idx: Index of the target user in the user_ids array.\n",
    "    - item_idx: Index of the target item.\n",
    "    - k: Number of nearest neighbors to consider.\n",
    "    - user_item_matrix: Sparse user-item matrix from the training set.\n",
    "    - user_ids: Array of user IDs corresponding to the rows in train_user_item_matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - The predicted rating.\n",
    "    \"\"\"\n",
    "    num_users = user_item_matrix.shape[0]\n",
    "    similarities = np.zeros(num_users)\n",
    "    \n",
    "    # Compute similarity between the target user and all other users in the training set\n",
    "    for other_user_idx in range(num_users):\n",
    "        if other_user_idx != user_idx:\n",
    "            user1_common_ratings, user2_common_ratings = get_dense_ratings(user_idx, other_user_idx, user_item_matrix)\n",
    "            similarity = manhattan_similarity(user1_common_ratings, user2_common_ratings)\n",
    "            similarities[other_user_idx] = similarity\n",
    "            # Use cosine similarity\n",
    "            # similarity = cosine_similarity (user1_common_ratings, user2_common_ratings)\n",
    "            # similarities[other_user_idx] = similarity\n",
    "    \n",
    "    # Sort users by similarity and select the top k similar users\n",
    "    top_k_users_indices = np.argsort(similarities)[-k:]\n",
    "    \n",
    "    # Calculate the weighted average of ratings from these top k similar users\n",
    "    top_k_similarities = similarities[top_k_users_indices]\n",
    "    # Ensure to extract ratings for the target item from top k users\n",
    "    top_k_ratings = user_item_matrix[top_k_users_indices, item_idx]\n",
    "    \n",
    "    # Compute the predicted rating as a weighted average\n",
    "    if np.sum(top_k_similarities) > 0:\n",
    "        predicted_rating = np.dot(top_k_ratings, top_k_similarities) / np.sum(top_k_similarities)\n",
    "    else:\n",
    "        # Fallback to the average rating for the item if no similarities are found\n",
    "        # Directly work with dense matrix, ensuring to consider only rated items\n",
    "        rated_item_indices = user_item_matrix[:, item_idx] > 0\n",
    "        if rated_item_indices.any():\n",
    "            predicted_rating = np.mean(user_item_matrix[rated_item_indices, item_idx])\n",
    "        else:\n",
    "            predicted_rating = 0  # Use a sensible default, like global average rating, if preferable\n",
    "    \n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_rmse_in_batches(testing_data, training_matrix, k, user_ids, item_ids, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Evaluates RMSE of the KNN model on the test data in batches.\n",
    "    \n",
    "    Args:\n",
    "    - testing_data: DataFrame containing the test set.\n",
    "    - train_matrix: User-item matrix for the training set, can be dense or sparse.\n",
    "    - k: Number of nearest neighbors to consider.\n",
    "    - user_ids: Array of user IDs from the original dataset, used to map users to matrix indices.\n",
    "    - item_ids: Array of item IDs from the original dataset, used to map items to matrix indices.\n",
    "    - batch_size: Number of user-item pairs to evaluate per batch, for efficiency.\n",
    "    \n",
    "    Returns:\n",
    "    - The RMSE for the test set.\n",
    "    \"\"\"\n",
    "    # Initialize list to store actual and predicted ratings\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    \n",
    "    # Mapping dictionaries for user and item IDs to their indices\n",
    "    user_indices = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "    item_indices = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "    \n",
    "    # Process in batches for efficiency\n",
    "    for start_idx in range(0, testing_data.shape[0], batch_size):\n",
    "        end_idx = min(start_idx + batch_size, testing_data.shape[0])\n",
    "        batch_data = testing_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        for _, row in batch_data.iterrows():\n",
    "            user_id, item_id, actual_rating = row['CustomerID'], row['MovieID'], row['Rating']\n",
    "            if user_id in user_indices and item_id in item_indices:\n",
    "                user_idx = user_indices[user_id]\n",
    "                item_idx = item_indices[item_id]\n",
    "                # Predict rating using the optimized function for dense matrices\n",
    "                predicted_rating = predict_rating(user_idx, item_idx, k, training_matrix, user_ids)\n",
    "                predicted_ratings.append(predicted_rating)\n",
    "                actual_ratings.append(actual_rating)\n",
    "    \n",
    "    # Calculate and return RMSE for the batch\n",
    "    rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for k=20: 3.624659741935849\n",
      "RMSE for k=25: 3.624659741935849\n",
      "RMSE for k=30: 3.624659741935849\n",
      "RMSE for k=35: 3.624659741935849\n",
      "RMSE for k=40: 3.624659741935849\n",
      "RMSE for k=45: 3.624659741935849\n",
      "RMSE for k=50: 3.624659741935849\n",
      "RMSE for k=100: 3.624579019979229\n",
      "RMSE for k=200: 3.624579019979229\n",
      "Best k: 100 with RMSE: 3.624579019979229\n"
     ]
    }
   ],
   "source": [
    "k_values = [20, 25, 30, 35, 40, 45, 50, 100, 200]\n",
    "\n",
    "best_k = None\n",
    "lowest_rmse = float('inf')\n",
    "\n",
    "# Iterate over each value of k\n",
    "for k in k_values:\n",
    "    # Use your existing function to calculate the RMSE for this k\n",
    "    current_rmse = evaluate_rmse_in_batches(validation_data, train_matrix, k, user_ids, item_ids, batch_size=1000)\n",
    "    \n",
    "    print(f'RMSE for k={k}: {current_rmse}')\n",
    "    \n",
    "    # Update best_k if the current model performs better\n",
    "    if current_rmse < lowest_rmse:\n",
    "        best_k = k\n",
    "        lowest_rmse = current_rmse\n",
    "\n",
    "print(f'Best k: {best_k} with RMSE: {lowest_rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 3.624579019979229\n"
     ]
    }
   ],
   "source": [
    "# Set the number of neighbors\n",
    "k = 200\n",
    "\n",
    "# Evaluate the model on the test set using the batch processing function\n",
    "rmse = evaluate_rmse_in_batches(validation_data,  train_matrix, k, user_ids, item_ids, batch_size=1000)\n",
    "\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
