{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>YearOfRelease</th>\n",
       "      <th>RatingYear</th>\n",
       "      <th>MovieAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1488844</td>\n",
       "      <td>3</td>\n",
       "      <td>2005-09-06</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>822109</td>\n",
       "      <td>5</td>\n",
       "      <td>2005-05-13</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>885013</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-10-19</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30878</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-12-26</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>823519</td>\n",
       "      <td>3</td>\n",
       "      <td>2004-05-03</td>\n",
       "      <td>2003</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID  CustomerID  Rating        Date  YearOfRelease  RatingYear  \\\n",
       "0        1     1488844       3  2005-09-06           2003        2005   \n",
       "1        1      822109       5  2005-05-13           2003        2005   \n",
       "2        1      885013       4  2005-10-19           2003        2005   \n",
       "3        1       30878       4  2005-12-26           2003        2005   \n",
       "4        1      823519       3  2004-05-03           2003        2004   \n",
       "\n",
       "   MovieAge  \n",
       "0         2  \n",
       "1         2  \n",
       "2         2  \n",
       "3         2  \n",
       "4         1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "training_df = pd.read_csv('C:/Users/nafla/OneDrive/Documents/system development/Netflix/training_data.csv')\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['CustomerID'] = training_df['CustomerID'].astype(str)\n",
    "training_df['MovieID'] = training_df['MovieID'].astype(str)\n",
    "training_df['Rating'] = pd.to_numeric(training_df['Rating'], errors='coerce')  # Converts to float, makes non-numeric as NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25     8.0\n",
      "0.50    24.0\n",
      "0.75    64.0\n",
      "Name: CustomerID, dtype: float64\n",
      "0.25     192.0\n",
      "0.50     552.5\n",
      "0.75    2539.0\n",
      "Name: MovieID, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate quantiles for user activity and item popularity\n",
    "user_activity_quantiles = training_df['CustomerID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "item_popularity_quantiles = training_df['MovieID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "print(user_activity_quantiles)\n",
    "print(item_popularity_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each user and item to a bin based on the quantiles\n",
    "training_df['UserActivityBin'] = pd.qcut(training_df.groupby('CustomerID')['Rating'].transform('size'), \n",
    "                                q=[0, .25, .5, .75, 1], labels=['low', 'medium', 'medium-high', 'high'])\n",
    "\n",
    "training_df['ItemPopularityBin'] = pd.qcut(training_df.groupby('MovieID')['Rating'].transform('size'), \n",
    "                                  q=[0, .25, .5, .75, 1], labels=['low', 'medium', 'medium-high', 'high'])\n",
    "\n",
    "# Combine these with Rating to create a stratification key\n",
    "training_df['Strata'] = training_df['UserActivityBin'].astype(str) + \"_\" + training_df['ItemPopularityBin'].astype(str) + \"_\" + training_df['Rating'].astype(str)\n",
    "\n",
    "# Perform stratified sampling\n",
    "# we use groupby and  frac to specify a fraction of each strata and in case number of rows is less that 10 it takes all rows\n",
    "strat_sample_df = training_df.groupby('Strata').apply(lambda x: x.sample(frac=0.001 if len(x) > 10 else len(x)/len(x))).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the sampled DataFrame: 24052\n"
     ]
    }
   ],
   "source": [
    "num_sampled_rows = len(strat_sample_df)\n",
    "print(f\"Number of rows in the sampled DataFrame: {num_sampled_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "training_data, testing_data = train_test_split(strat_sample_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# further split the training data into training and validation sets\n",
    "training_data, validation_data = train_test_split(training_data, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the user-item matrix using pivot_table\n",
    "user_item_matrix = training_data.pivot_table(index='CustomerID', columns='MovieID', values='Rating', fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pivot table to a numpy array if you need a dense matrix format\n",
    "dense_matrix = user_item_matrix.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the matrix so that each user vector has unit norm\n",
    "user_item_matrix_normalized = normalize(dense_matrix, axis=1, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_ratings(user1_idx, user2_idx, user_item_matrix):\n",
    "    \"\"\"\n",
    "    Extracts dense rating vectors for common items rated by both users.\n",
    "    \"\"\"\n",
    "    user1_ratings = user_item_matrix[user1_idx, :]\n",
    "    user2_ratings = user_item_matrix[user2_idx, :]\n",
    "    \n",
    "    common_item_indices = np.where((user1_ratings > 0) & (user2_ratings > 0))[0]\n",
    "    \n",
    "    return user1_ratings[common_item_indices], user2_ratings[common_item_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# def cosine_similarity(user1_common_ratings, user2_common_ratings):\n",
    "#     \"\"\"\n",
    "#     Computes the cosine similarity between two users' ratings for common items.\n",
    "    \n",
    "#     Args:\n",
    "#     - user1_common_ratings, user2_common_ratings: Dense rating vectors for common items.\n",
    "    \n",
    "#     Returns:\n",
    "#     - The computed similarity score.\n",
    "#     \"\"\"\n",
    "#     # Prevent division by zero\n",
    "#     if len(user1_common_ratings) == 0 or len(user2_common_ratings) == 0:\n",
    "#         return 0\n",
    "#     # Compute the cosine distance and convert it to similarity\n",
    "#     similarity = 1 - cosine(user1_common_ratings, user2_common_ratings)\n",
    "#     return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Calculate the cosine similarity matrix\n",
    "user_similarity_matrix = cosine_similarity(dense_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_rating(user_idx, item_idx, k, cosine_similarity_matrix, user_item_matrix):\n",
    "    # Get the similarities for the target user\n",
    "    user_similarities = cosine_similarity_matrix[user_idx]\n",
    "    \n",
    "    # Get indices of the top k most similar users, excluding the user itself\n",
    "    similar_users_indices = np.argsort(user_similarities)[-k-1:-1][::-1]\n",
    "    \n",
    "    # Calculate the predicted rating using the similarities and ratings of the top k similar users\n",
    "    top_k_similarities = user_similarities[similar_users_indices]\n",
    "    top_k_users_ratings = user_item_matrix[similar_users_indices, item_idx]\n",
    "    \n",
    "    # Check for NaN values in top_k_similarities\n",
    "    if np.isnan(top_k_similarities).any():\n",
    "        print(f\"NaN found in similarities for user index {user_idx} and item index {item_idx}.\")\n",
    "        # Handle the NaN values, for example, by setting them to 0 or ignoring them\n",
    "        top_k_similarities = np.nan_to_num(top_k_similarities)\n",
    "    \n",
    "    # Predict rating \n",
    "    rated = top_k_users_ratings > 0\n",
    "    if rated.any() and np.sum(top_k_similarities[rated]) > 0:\n",
    "        predicted_rating = np.dot(top_k_users_ratings[rated], top_k_similarities[rated]) / np.sum(top_k_similarities[rated])\n",
    "    else:\n",
    "        predicted_rating = 0  # Or use another fallback such as global or user's average rating\n",
    "    \n",
    "    return predicted_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def predict_rating_vectorized(user_idx, item_idx, k, cosine_similarity_matrix, user_item_matrix):\n",
    "#     # Get the similarities for the target user\n",
    "#     user_similarities = cosine_similarity_matrix[user_idx]\n",
    "    \n",
    "#     # Get indices of the top k most similar users, excluding the user itself\n",
    "#     similar_users_indices = np.argsort(user_similarities)[-k-1:-1][::-1]\n",
    "    \n",
    "#     # Calculate the predicted rating using the similarities and ratings of the top k similar users\n",
    "#     top_k_similarities = user_similarities[similar_users_indices]\n",
    "#     top_k_users_ratings = user_item_matrix[similar_users_indices, item_idx]\n",
    "    \n",
    "#     # Check for NaN values in top_k_similarities\n",
    "#     if np.isnan(top_k_similarities).any():\n",
    "#         print(f\"NaN found in similarities for user index {user_idx} and item index {item_idx}.\")\n",
    "#         # Handle the NaN values, for example, by setting them to 0 or ignoring them\n",
    "#         top_k_similarities = np.nan_to_num(top_k_similarities)\n",
    "    \n",
    "#     # Continue with your logic...\n",
    "#     rated = top_k_users_ratings > 0\n",
    "#     if rated.any() and np.sum(top_k_similarities[rated]) > 0:\n",
    "#         predicted_rating = np.dot(top_k_users_ratings[rated], top_k_similarities[rated]) / np.sum(top_k_similarities[rated])\n",
    "#     else:\n",
    "#         predicted_rating = 0  # Or use another fallback such as global or user's average rating\n",
    "    \n",
    "#     return predicted_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_rmse_in_batches(testing_data, cosine_similarity_matrix, user_item_matrix, k, user_ids, item_ids, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Evaluates RMSE of the KNN model on the test data in batches.\n",
    "    \n",
    "    Args:\n",
    "    - testing_data: DataFrame containing the test set.\n",
    "    - train_matrix: User-item matrix for the training set, can be dense or sparse.\n",
    "    - k: Number of nearest neighbors to consider.\n",
    "    - user_ids: Array of user IDs from the original dataset, used to map users to matrix indices.\n",
    "    - item_ids: Array of item IDs from the original dataset, used to map items to matrix indices.\n",
    "    - batch_size: Number of user-item pairs to evaluate per batch, for efficiency.\n",
    "    \n",
    "    Returns:\n",
    "    - The RMSE for the test set.\n",
    "    \"\"\"\n",
    "    # Initialize list to store actual and predicted ratings\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    \n",
    "    # Mapping dictionaries for user and item IDs to their indices\n",
    "    user_indices = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "    item_indices = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "    \n",
    "    # Process in batches for efficiency\n",
    "    for start_idx in range(0, testing_data.shape[0], batch_size):\n",
    "        end_idx = min(start_idx + batch_size, testing_data.shape[0])\n",
    "        batch_data = testing_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        for _, row in batch_data.iterrows():\n",
    "            user_id, item_id, actual_rating = row['CustomerID'], row['MovieID'], row['Rating']\n",
    "            if user_id in user_indices and item_id in item_indices:\n",
    "                user_idx = user_indices[user_id]\n",
    "                item_idx = item_indices[item_id]\n",
    "                # Predict rating using the optimized function for dense matrices\n",
    "                predicted_rating = predict_rating(user_idx, item_idx, k, cosine_similarity_matrix, user_item_matrix)\n",
    "                predicted_ratings.append(predicted_rating)\n",
    "                actual_ratings.append(actual_rating)\n",
    "    \n",
    "    # Calculate and return RMSE for the batch\n",
    "    rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_values = [20, 25, 30, 35, 40, 45, 50, 100, 200]\n",
    "\n",
    "# best_k = None\n",
    "# lowest_rmse = float('inf')\n",
    "\n",
    "# # Ensure the `cosine_similarity_matrix` and `user_item_matrix` are correctly prepared\n",
    "# # `user_ids` and `item_ids` should correctly map users and items to their indices in these matrices\n",
    "\n",
    "# # Iterate over each value of k\n",
    "# for k in k_values:\n",
    "#     # Calculate the RMSE for this k using the validation data\n",
    "#     # Assuming the evaluate_rmse_in_batches function signature is updated to accept the cosine similarity matrix\n",
    "#     current_rmse = evaluate_rmse_in_batches2(validation_data, cosine_similarity_matrix, k, user_item_matrix_normalized, user_ids, item_ids, batch_size=1000)\n",
    "    \n",
    "#     print(f'RMSE for k={k} on validation set: {current_rmse}')\n",
    "    \n",
    "#     # Update best_k if the current model performs better on the validation set\n",
    "#     if current_rmse < lowest_rmse:\n",
    "#         best_k = k\n",
    "#         lowest_rmse = current_rmse\n",
    "\n",
    "# print(f'Best k: {best_k} with RMSE: {lowest_rmse} on validation set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 3.717630861285077\n"
     ]
    }
   ],
   "source": [
    "# Set the number of neighbors\n",
    "k = 100\n",
    "\n",
    "# Defining user and item ids in training data\n",
    "user_ids = training_data['CustomerID'].unique()\n",
    "item_ids = training_data['MovieID'].unique()\n",
    "\n",
    "# Evaluate the model on the test set using the batch processing function\n",
    "rmse = evaluate_rmse_in_batches(validation_data,  user_similarity_matrix, user_item_matrix_normalized, k, user_ids, item_ids, batch_size=1000)\n",
    "\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
