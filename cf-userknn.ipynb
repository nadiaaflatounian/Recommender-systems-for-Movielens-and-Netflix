{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>YearOfRelease</th>\n",
       "      <th>RatingYear</th>\n",
       "      <th>MovieAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1488844</td>\n",
       "      <td>3</td>\n",
       "      <td>2005-09-06</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>822109</td>\n",
       "      <td>5</td>\n",
       "      <td>2005-05-13</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>885013</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-10-19</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30878</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-12-26</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>823519</td>\n",
       "      <td>3</td>\n",
       "      <td>2004-05-03</td>\n",
       "      <td>2003</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID  CustomerID  Rating        Date  YearOfRelease  RatingYear  \\\n",
       "0        1     1488844       3  2005-09-06           2003        2005   \n",
       "1        1      822109       5  2005-05-13           2003        2005   \n",
       "2        1      885013       4  2005-10-19           2003        2005   \n",
       "3        1       30878       4  2005-12-26           2003        2005   \n",
       "4        1      823519       3  2004-05-03           2003        2004   \n",
       "\n",
       "   MovieAge  \n",
       "0         2  \n",
       "1         2  \n",
       "2         2  \n",
       "3         2  \n",
       "4         1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "training_df = pd.read_csv('C:/Users/nafla/OneDrive/Documents/system development/Netflix/training_data.csv')\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25     8.0\n",
      "0.50    24.0\n",
      "0.75    64.0\n",
      "Name: CustomerID, dtype: float64\n",
      "0.25     192.0\n",
      "0.50     552.5\n",
      "0.75    2539.0\n",
      "Name: MovieID, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate quantiles for user activity and item popularity\n",
    "user_activity_quantiles = training_df['CustomerID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "item_popularity_quantiles = training_df['MovieID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "print(user_activity_quantiles)\n",
    "print(item_popularity_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['CustomerID'] = training_df['CustomerID'].astype(str)\n",
    "training_df['MovieID'] = training_df['MovieID'].astype(str)\n",
    "training_df['Rating'] = pd.to_numeric(training_df['Rating'], errors='coerce')  # Converts to float, makes non-numeric as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before conversion:\n",
      "MovieID          object\n",
      "CustomerID       object\n",
      "Rating            int64\n",
      "Date             object\n",
      "YearOfRelease     int64\n",
      "RatingYear        int64\n",
      "MovieAge          int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(\"Before conversion:\")\n",
    "print(training_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Sampling Method \n",
    "\n",
    "To create a representative sample of our dataset, we employ a stratified sampling method that accounts for three key dimensions: Rating Distribution, User Activity, and Item Popularity. This approach ensures our sample maintains the diversity and characteristics of the entire dataset, facilitating more reliable model training and evaluation.\n",
    "\n",
    "- User Activity is quantified by the number of ratings a user has provided.\n",
    "- Item Popularity reflects the number of ratings an item has received.\n",
    "\n",
    "Finally, We combine User Activity, Item Popularity, and Rating into a composite stratification key for each record. This multi-dimensional key ensures our sampling process considers the distribution across all three axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each user and item to a bin based on the quantiles\n",
    "training_df['UserActivityBin'] = pd.qcut(training_df.groupby('CustomerID')['Rating'].transform('size'), \n",
    "                                q=[0, .25, .5, .75, 1], labels=['low', 'medium', 'medium-high', 'high'])\n",
    "\n",
    "training_df['ItemPopularityBin'] = pd.qcut(training_df.groupby('MovieID')['Rating'].transform('size'), \n",
    "                                  q=[0, .25, .5, .75, 1], labels=['low', 'medium', 'medium-high', 'high'])\n",
    "\n",
    "# Combine these with Rating to create a stratification key\n",
    "training_df['Strata'] = training_df['UserActivityBin'].astype(str) + \"_\" + training_df['ItemPopularityBin'].astype(str) + \"_\" + training_df['Rating'].astype(str)\n",
    "\n",
    "# Perform stratified sampling\n",
    "# we use groupby and  frac to specify a fraction of each strata and in case number of rows is less that 10 it takes all rows\n",
    "strat_sample_df = training_df.groupby('Strata').apply(lambda x: x.sample(frac=0.005 if len(x) > 10 else len(x)/len(x))).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the sampled DataFrame: 120269\n"
     ]
    }
   ],
   "source": [
    "num_sampled_rows = len(strat_sample_df)\n",
    "print(f\"Number of rows in the sampled DataFrame: {num_sampled_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset to training, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming strat_sample_df is your entire dataset\n",
    "user_ids = strat_sample_df['CustomerID'].unique()\n",
    "movie_ids = strat_sample_df['MovieID'].unique()\n",
    "\n",
    "# Create mappings based on the entire dataset\n",
    "user_id_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "movie_id_to_index = {movie_id: index for index, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "# Now, split your dataset\n",
    "training_data, testing_data = train_test_split(strat_sample_df, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ids_to_indices(df, user_id_to_index, movie_id_to_index):\n",
    "    \"\"\"\n",
    "    Map user IDs and movie IDs to their respective indices.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing 'CustomerID', 'MovieID', and other columns.\n",
    "    - user_id_to_index: Dictionary mapping user IDs to indices.\n",
    "    - movie_id_to_index: Dictionary mapping movie IDs to indices.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with added columns 'UserIndex' and 'MovieIndex' for the respective indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    modified_data = df.copy()\n",
    "    \n",
    "    # Map 'CustomerID' to 'UserIndex'\n",
    "    modified_data['UserIndex'] = modified_data['CustomerID'].map(user_id_to_index)\n",
    "    \n",
    "    # Map 'MovieID' to 'MovieIndex'\n",
    "    modified_data['MovieIndex'] = modified_data['MovieID'].map(movie_id_to_index)\n",
    "    \n",
    "    # Optional: drop rows where either UserIndex or MovieIndex is NaN (i.e., ID wasn't found)\n",
    "    modified_data.dropna(subset=['UserIndex', 'MovieIndex'], inplace=True)\n",
    "    \n",
    "    # Convert indices to integers (they might be floats due to NaN handling)\n",
    "    modified_data['UserIndex'] = modified_data['UserIndex'].astype(int)\n",
    "    modified_data['MovieIndex'] = modified_data['MovieIndex'].astype(int)\n",
    "    \n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_training_data = map_ids_to_indices(training_data,user_id_to_index, movie_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 96215\n",
      "Training Data Size: 96215\n",
      "Testing Data Size: 24054\n"
     ]
    }
   ],
   "source": [
    "# Calculate the size of each split\n",
    "training_size = training_data.shape[0]  # Number of rows in the training data\n",
    "training_size_mapp = mapped_training_data.shape[0]  # Number of rows in the training data\n",
    "# validation_size = validation_data.shape[0]  # Number of rows in the validation data\n",
    "testing_size = testing_data.shape[0]  # Number of rows in the testing data\n",
    "\n",
    "# Print the sizes\n",
    "print(f\"Training Data Size: {training_size}\")\n",
    "print(f\"Training Data Size: {training_size_mapp}\")\n",
    "# print(f\"Validation Data Size: {validation_size}\")\n",
    "print(f\"Testing Data Size: {testing_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique CustomerIDs in Training Data: 74675\n",
      "Unique MovieIDs in Testing Data: 22352\n"
     ]
    }
   ],
   "source": [
    "# Assuming final_training_data, validation_data, and testing_data are your data splits\n",
    "\n",
    "# Count unique MovieIDs in the final training data\n",
    "unique_users_training = mapped_training_data['CustomerID'].nunique()\n",
    "\n",
    "# Count unique MovieIDs in the validation data\n",
    "# unique_movies_validation = validation_data['MovieID'].nunique()\n",
    "\n",
    "# Count unique MovieIDs in the testing data\n",
    "unique_movies_testing = testing_data['CustomerID'].nunique()\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Unique CustomerIDs in Training Data: {unique_users_training}\")\n",
    "# print(f\"Unique MovieIDs in Validation Data: {unique_movies_validation}\")\n",
    "print(f\"Unique MovieIDs in Testing Data: {unique_movies_testing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating User - Item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 25)\t3\n",
      "  (0, 2383)\t4\n",
      "  (0, 2522)\t3\n",
      "  (1, 0)\t1\n",
      "  (1, 44)\t5\n",
      "  (1, 2369)\t3\n",
      "  (2, 1)\t1\n",
      "  (3, 2664)\t4\n",
      "  (4, 0)\t1\n",
      "  (4, 1912)\t4\n",
      "  (5, 3)\t1\n",
      "  (5, 18)\t3\n",
      "  (6, 4)\t1\n",
      "  (6, 532)\t1\n",
      "  (6, 1151)\t4\n",
      "  (7, 0)\t1\n",
      "  (8, 5)\t1\n",
      "  (9, 6)\t1\n",
      "  (10, 0)\t1\n",
      "  (10, 2562)\t5\n",
      "  (11, 7)\t1\n",
      "  (12, 8)\t1\n",
      "  (12, 399)\t3\n",
      "  (12, 1470)\t3\n",
      "  (13, 9)\t1\n",
      "  :\t:\n",
      "  (88611, 2610)\t5\n",
      "  (88612, 2728)\t5\n",
      "  (88613, 2641)\t5\n",
      "  (88614, 2572)\t5\n",
      "  (88615, 2566)\t5\n",
      "  (88616, 2610)\t5\n",
      "  (88617, 2624)\t5\n",
      "  (88618, 2637)\t5\n",
      "  (88619, 2645)\t5\n",
      "  (88620, 2659)\t5\n",
      "  (88622, 2690)\t5\n",
      "  (88623, 2597)\t5\n",
      "  (88625, 2654)\t5\n",
      "  (88626, 2486)\t5\n",
      "  (88627, 2540)\t5\n",
      "  (88628, 2566)\t5\n",
      "  (88629, 2634)\t5\n",
      "  (88631, 2687)\t5\n",
      "  (88632, 2697)\t5\n",
      "  (88633, 2719)\t5\n",
      "  (88634, 2617)\t5\n",
      "  (88635, 2487)\t5\n",
      "  (88636, 2607)\t5\n",
      "  (88637, 2575)\t5\n",
      "  (88638, 2556)\t5\n"
     ]
    }
   ],
   "source": [
    "# Creating customer-movie matrix\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# user_ids = training_data['CustomerID'].unique()\n",
    "# movie_ids = training_data['MovieID'].unique()\n",
    "\n",
    "# user_id_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "# movie_id_to_index = {movie_id: index for index, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "\n",
    "# Extract rows, columns, and data for CSR matrix\n",
    "# rows = training_data['UserIndex'].values\n",
    "# cols = training_data['MovieIndex'].values\n",
    "# data = training_data['Rating'].values\n",
    "\n",
    "# # Calculate the shape of the matrix\n",
    "# num_users = len(user_id_to_index)\n",
    "# num_movies = len(movie_id_to_index)\n",
    "\n",
    "# # Create the CSR matrix\n",
    "# ratings_csr_matrix = csr_matrix((data, (rows, cols)), shape=(num_users, num_movies))\n",
    "\n",
    "# print(ratings_csr_matrix)\n",
    "\n",
    "# Extract the rows (user indices), columns (movie indices), and data (ratings) for the CSR matrix\n",
    "rows = mapped_training_data['UserIndex'].values\n",
    "cols = mapped_training_data['MovieIndex'].values\n",
    "data = mapped_training_data['Rating'].values\n",
    "\n",
    "# Determine the shape of the CSR matrix\n",
    "# The shape is (max_user_index + 1, max_movie_index + 1) because indices start from 0\n",
    "num_users = mapped_training_data['UserIndex'].max() + 1\n",
    "num_movies = mapped_training_data['MovieIndex'].max() + 1\n",
    "\n",
    "# Create the CSR matrix\n",
    "ratings_csr_matrix = csr_matrix((data, (rows, cols)), shape=(num_users, num_movies))\n",
    "\n",
    "print(ratings_csr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define similarity function for each given user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "cosine_similarity_matrix_csr = cosine_similarity(ratings_csr_matrix, dense_output=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 87742)\t0.5144957554275265\n",
      "  (0, 87336)\t0.5144957554275265\n",
      "  (0, 86833)\t0.5144957554275265\n",
      "  (0, 86712)\t0.5144957554275265\n",
      "  (0, 86050)\t0.5144957554275265\n",
      "  (0, 85568)\t0.5144957554275265\n",
      "  (0, 85558)\t0.5144957554275265\n",
      "  (0, 85116)\t0.5144957554275265\n",
      "  (0, 84743)\t0.5144957554275265\n",
      "  (0, 84532)\t0.5144957554275265\n",
      "  (0, 84099)\t0.5144957554275265\n",
      "  (0, 83997)\t0.5144957554275265\n",
      "  (0, 83965)\t0.4601789933084222\n",
      "  (0, 83920)\t0.5144957554275265\n",
      "  (0, 83897)\t0.5144957554275265\n",
      "  (0, 83696)\t0.5144957554275265\n",
      "  (0, 83610)\t0.5144957554275265\n",
      "  (0, 81093)\t0.5144957554275265\n",
      "  (0, 79938)\t0.3086974532565159\n",
      "  (0, 78973)\t0.36380343755449945\n",
      "  (0, 74019)\t0.41159660434202117\n",
      "  (0, 73188)\t0.4411764705882352\n",
      "  (0, 70513)\t0.15339299776947407\n",
      "  (0, 68433)\t0.3086974532565159\n",
      "  (0, 67255)\t0.2300894966542111\n",
      "  :\t:\n",
      "  (88638, 13777)\t1.0\n",
      "  (88638, 13775)\t1.0\n",
      "  (88638, 13218)\t1.0\n",
      "  (88638, 13104)\t1.0\n",
      "  (88638, 12745)\t1.0\n",
      "  (88638, 12694)\t1.0\n",
      "  (88638, 12340)\t0.3713906763541037\n",
      "  (88638, 11232)\t0.7071067811865475\n",
      "  (88638, 10348)\t0.5144957554275265\n",
      "  (88638, 9993)\t0.9486832980505138\n",
      "  (88638, 9748)\t0.5144957554275265\n",
      "  (88638, 7444)\t1.0\n",
      "  (88638, 5334)\t0.19611613513818404\n",
      "  (88638, 5048)\t0.8574929257125441\n",
      "  (88638, 4998)\t0.6396021490668313\n",
      "  (88638, 4855)\t0.48507125007266594\n",
      "  (88638, 4471)\t0.7071067811865475\n",
      "  (88638, 4287)\t0.4472135954999579\n",
      "  (88638, 3842)\t0.6868028197434451\n",
      "  (88638, 3824)\t0.9805806756909202\n",
      "  (88638, 3777)\t0.565685424949238\n",
      "  (88638, 2526)\t1.0\n",
      "  (88638, 2303)\t0.4242640687119285\n",
      "  (88638, 2185)\t0.7071067811865476\n",
      "  (88638, 1974)\t0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity_matrix_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88639, 88639)\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity_matrix_csr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict ratings using similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating_with_similarity_matrix(csr_user_item_matrix, similarity_matrix, user_index, movie_index, k):\n",
    "    \"\"\"\n",
    "    Predict the rating for a given movie by a target user, based on the ratings of top-k similar users.\n",
    "    This function uses a pre-calculated similarity matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - csr_user_item_matrix: CSR matrix representing the user-item matrix.\n",
    "    - similarity_matrix: CSR matrix representing the similarity scores between users.\n",
    "    - user_index: The index of the user for whom the rating is being predicted.\n",
    "    - movie_index: The index of the movie for which the rating is being predicted.\n",
    "    - k: Number of top similar users to consider for prediction.\n",
    "    \n",
    "    Returns:\n",
    "    - Predicted rating for the movie by the target user.\n",
    "    \"\"\"\n",
    "    # Step 1: Identify users who have rated the movie\n",
    "    movie_rated_indices = csr_user_item_matrix[:, movie_index].nonzero()[0]\n",
    "    \n",
    "    # Check if user_index exists in the similarity matrix\n",
    "    if user_index < 0 or user_index >= similarity_matrix.shape[0]:\n",
    "        # If user_index does not exist in similarity matrix, return default prediction\n",
    "        overall_average_rating = csr_user_item_matrix[:, movie_index].data.mean()\n",
    "        return overall_average_rating if np.isfinite(overall_average_rating) else 3.0  # Assuming 3.0 as a neutral rating\n",
    "    \n",
    "    # Step 2: Extract similarity scores for the target user with all other users\n",
    "    user_similarities = similarity_matrix.getrow(user_index).toarray().flatten()\n",
    "    \n",
    "    # Step 3: Filter the similarities for users who have rated the movie\n",
    "    filtered_similarities = user_similarities[movie_rated_indices]\n",
    "    \n",
    "    # Step 4: Get indices of top k similar users among those who have rated the movie\n",
    "    top_k_indices = np.argsort(filtered_similarities)[-k:]\n",
    "    top_k_users_indices = movie_rated_indices[top_k_indices]\n",
    "    top_k_similarities = filtered_similarities[top_k_indices]\n",
    "\n",
    "    # Retrieve ratings for the movie from these top-k similar users\n",
    "    top_k_ratings = csr_user_item_matrix[top_k_users_indices, movie_index].toarray().flatten()\n",
    "\n",
    "    # Calculate the weighted average rating\n",
    "    weighted_sum = np.dot(top_k_similarities, top_k_ratings)\n",
    "    similarity_sum = np.sum(top_k_similarities)\n",
    "    \n",
    "    if similarity_sum > 0:\n",
    "        predicted_rating = weighted_sum / similarity_sum\n",
    "    else:\n",
    "        # Use the overall average rating of the movie by all users as the default rating\n",
    "        overall_average_rating = csr_user_item_matrix[:, movie_index].data.mean()\n",
    "        predicted_rating = overall_average_rating if np.isfinite(overall_average_rating) else 3.0  # Assuming 3.0 as a neutral rating\n",
    "\n",
    "    return predicted_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def evaluate_predictions_csr(validation_data, csr_user_item_matrix, similarity_matrix, k):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendation system by predicting ratings for each user-movie pair in the validation set\n",
    "    using a CSR matrix and pre-computed similarity matrix, and comparing the predictions to the actual ratings using RMSE.\n",
    "\n",
    "    Parameters:\n",
    "    - validation_data: DataFrame containing 'UserIndex', 'MovieIndex', and 'Rating'.\n",
    "    - csr_user_item_matrix: CSR matrix representing the user-item matrix from the training set.\n",
    "    - similarity_matrix: Pre-computed similarity matrix as a CSR matrix.\n",
    "    - k: The number of top similar users to consider when making predictions.\n",
    "    \n",
    "    Returns:\n",
    "    - rmse: The root mean square error of the predicted ratings against the actual ratings.\n",
    "    \"\"\"\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for _, row in validation_data.iterrows():\n",
    "        user_index = row['UserIndex']\n",
    "        movie_index = row['MovieIndex']\n",
    "        actual_rating = row['Rating']\n",
    "        \n",
    "        # Check if the movie index is valid\n",
    "        if movie_index in csr_user_item_matrix.indices:\n",
    "            predicted_rating = predict_rating_with_similarity_matrix(csr_user_item_matrix, similarity_matrix, user_index, movie_index, k)\n",
    "            actual_ratings.append(actual_rating)\n",
    "            predicted_ratings.append(predicted_rating)\n",
    "    \n",
    "    # Calculate RMSE between actual and predicted ratings\n",
    "    actual_ratings = np.array(actual_ratings)\n",
    "    predicted_ratings = np.array(predicted_ratings)\n",
    "    valid_mask = ~np.isnan(predicted_ratings)\n",
    "    rmse = sqrt(mean_squared_error(actual_ratings[valid_mask], predicted_ratings[valid_mask]))\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ids_to_indices(df, user_id_to_index, movie_id_to_index):\n",
    "    \"\"\"\n",
    "    Map user IDs and movie IDs to their respective indices.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing 'CustomerID', 'MovieID', and other columns.\n",
    "    - user_id_to_index: Dictionary mapping user IDs to indices.\n",
    "    - movie_id_to_index: Dictionary mapping movie IDs to indices.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with added columns 'UserIndex' and 'MovieIndex' for the respective indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    modified_data = df.copy()\n",
    "    \n",
    "    # Map 'CustomerID' to 'UserIndex'\n",
    "    modified_data['UserIndex'] = modified_data['CustomerID'].map(user_id_to_index)\n",
    "    \n",
    "    # Map 'MovieID' to 'MovieIndex'\n",
    "    modified_data['MovieIndex'] = modified_data['MovieID'].map(movie_id_to_index)\n",
    "    \n",
    "    # Optional: drop rows where either UserIndex or MovieIndex is NaN (i.e., ID wasn't found)\n",
    "    modified_data.dropna(subset=['UserIndex', 'MovieIndex'], inplace=True)\n",
    "    \n",
    "    # Convert indices to integers (they might be floats due to NaN handling)\n",
    "    modified_data['UserIndex'] = modified_data['UserIndex'].astype(int)\n",
    "    modified_data['MovieIndex'] = modified_data['MovieIndex'].astype(int)\n",
    "    \n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_train_data = map_ids_to_indices (training_data, user_id_to_index, movie_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5, Average RMSE=1.036700203350159\n",
      "k=20, Average RMSE=1.0673296373556096\n",
      "k=50, Average RMSE=1.0153982283256808\n",
      "k=100, Average RMSE=0.9958867788327659\n",
      "k=200, Average RMSE=0.9747812369560493\n",
      "Best k: 200 with RMSE: 0.9747812369560493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming k_values to test and your similarity matrix is already defined\n",
    "k_values = [5, 20, 50, 100, 200]\n",
    "similarity_matrix = cosine_similarity_matrix_csr\n",
    "# Setup KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    fold_rmses = []  # Store RMSEs for each fold\n",
    "\n",
    "    for train_indices, test_indices in kf.split(ratings_csr_matrix):\n",
    "        # Splitting your data: ratings_csr_matrix doesn't change, so you just map validation set indices\n",
    "        validation_data_fold = mapped_train_data.iloc[test_indices]\n",
    "\n",
    "        # Evaluate predictions on this fold's test set\n",
    "        rmse = evaluate_predictions_csr(validation_data_fold, ratings_csr_matrix, similarity_matrix, k)\n",
    "        fold_rmses.append(rmse)\n",
    "\n",
    "    # Calculate average RMSE for this k over all folds\n",
    "    avg_rmse = np.mean(fold_rmses)\n",
    "    results.append((k, avg_rmse))\n",
    "    print(f\"k={k}, Average RMSE={avg_rmse}\")\n",
    "\n",
    "# Find the best k value based on average RMSE\n",
    "best_k, best_rmse = min(results, key=lambda x: x[1])\n",
    "print(f\"Best k: {best_k} with RMSE: {best_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_testing_data = map_ids_to_indices(testing_data, user_id_to_index, movie_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0615366311315348\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "similarity_matrix = cosine_similarity_matrix_csr\n",
    "k = 200  # Example value for k\n",
    "rmse = evaluate_predictions_csr(mapped_testing_data, ratings_csr_matrix, similarity_matrix, k)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, user_item_matrix, similarity_matrix, df, N=10):\n",
    "    \"\"\"\n",
    "    Recommend top N movies for a given user using a pre-calculated similarity matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: The ID of the user for whom to generate recommendations.\n",
    "    - user_item_matrix: DataFrame representing the user-item matrix (users as rows, movies as columns).\n",
    "    - similarity_matrix: DataFrame representing the pre-calculated similarities between users.\n",
    "    - movie_titles: DataFrame or Series mapping MovieIDs to movie titles.\n",
    "    - N: Number of movies to recommend.\n",
    "\n",
    "    Returns:\n",
    "    - A list of tuples with (MovieID, Movie Title, Predicted Rating) for the top N recommended movies.\n",
    "    \"\"\"\n",
    "    # Ensure user_id is the correct type\n",
    "    user_id = str(user_id)\n",
    "    \n",
    "    # Get the top 25 most similar users to the target user\n",
    "    top_25_users = similarity_matrix.loc[user_id].sort_values(ascending=False).head(25).index\n",
    "    \n",
    "    # Predict ratings for movies the user hasn't seen\n",
    "    predicted_ratings = {}\n",
    "    for movie_id in user_item_matrix.columns:\n",
    "        # Skip if the user has already rated this movie\n",
    "        if not pd.isna(user_item_matrix.at[user_id, movie_id]) and user_item_matrix.at[user_id, movie_id] != 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the weighted average of ratings from the top 25 similar users\n",
    "        total_weight = 0\n",
    "        weighted_sum = 0\n",
    "        for similar_user in top_25_users:\n",
    "            # Check if the similar user has rated the movie\n",
    "            if pd.isna(user_item_matrix.at[similar_user, movie_id]) or user_item_matrix.at[similar_user, movie_id] == 0:\n",
    "                continue\n",
    "            similarity_score = similarity_matrix.at[user_id, similar_user]\n",
    "            rating = user_item_matrix.at[similar_user, movie_id]\n",
    "            weighted_sum += similarity_score * rating\n",
    "            total_weight += similarity_score\n",
    "        \n",
    "        # Predict the rating if there were any weights, otherwise default to 0\n",
    "        predicted_rating = weighted_sum / total_weight if total_weight > 0 else 0\n",
    "        predicted_ratings[movie_id] = predicted_rating\n",
    "    \n",
    "    # Sort the predicted ratings and select the top N\n",
    "    top_n_recommendations = sorted(predicted_ratings.items(), key=lambda x: x[1], reverse=True)[:N]\n",
    "    \n",
    "    # Fetch the titles for the recommended movies\n",
    "    recommendations = [(movie_id, df[movie_id], rating) for movie_id, rating in top_n_recommendations]\n",
    "    \n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies2(user_id, user_item_matrix, similarity_matrix, N=10):\n",
    "    \"\"\"\n",
    "    Recommend top N movies for a given user using a pre-calculated similarity matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: The ID of the user for whom to generate recommendations.\n",
    "    - user_item_matrix: CSR matrix representing the user-item interactions matrix.\n",
    "    - similarity_matrix: CSR matrix representing the pre-calculated similarities between users.\n",
    "    - N: Number of movies to recommend.\n",
    "\n",
    "    Returns:\n",
    "    - A list of tuples with (MovieID, Predicted Rating) for the top N recommended movies.\n",
    "    \"\"\"\n",
    "    # Get the row corresponding to the user_id\n",
    "    user_index = int(user_id)\n",
    "    similarity_row = similarity_matrix.getrow(user_index)\n",
    "    \n",
    "    # Get indices of top similar users\n",
    "    similar_users_indices = similarity_row.indices[:200]  # Get indices of top 200 similar users\n",
    "    \n",
    "    # Predict ratings for movies the user hasn't seen\n",
    "    predicted_ratings = {}\n",
    "    for movie_id in range(user_item_matrix.shape[1]):\n",
    "        # Skip if the user has already rated this movie\n",
    "        if user_item_matrix[user_index, movie_id] != 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the weighted average of ratings from the top similar users\n",
    "        total_weight = 0\n",
    "        weighted_sum = 0\n",
    "        for similar_user_index in similar_users_indices:\n",
    "            similarity_score = similarity_matrix[user_index, similar_user_index]\n",
    "            rating = user_item_matrix[similar_user_index, movie_id]\n",
    "            weighted_sum += similarity_score * rating\n",
    "            total_weight += similarity_score\n",
    "        \n",
    "        # Predict the rating if there were any weights, otherwise default to 0\n",
    "        predicted_rating = weighted_sum / total_weight if total_weight > 0 else 0\n",
    "        predicted_ratings[movie_id] = predicted_rating\n",
    "    \n",
    "    # Sort the predicted ratings and select the top N\n",
    "    top_n_recommendations = sorted(predicted_ratings.items(), key=lambda x: x[1], reverse=True)[:N]\n",
    "    \n",
    "    return top_n_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_mapped_data = map_ids_to_indices(strat_sample_df, user_id_to_index, movie_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the rows (user indices), columns (movie indices), and data (ratings) for the CSR matrix\n",
    "rows = main_mapped_data['UserIndex'].values\n",
    "cols = main_mapped_data['MovieIndex'].values\n",
    "data = main_mapped_data['Rating'].values\n",
    "\n",
    "# Determine the shape of the CSR matrix\n",
    "# The shape is (max_user_index + 1, max_movie_index + 1) because indices start from 0\n",
    "num_users = main_mapped_data['UserIndex'].max() + 1\n",
    "num_movies = main_mapped_data['MovieIndex'].max() + 1\n",
    "\n",
    "# Create the CSR matrix\n",
    "Main_csr_matrix = csr_matrix((data, (rows, cols)), shape=(num_users, num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_similarity_matrix = cosine_similarity(Main_csr_matrix, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88639, 88639)\n"
     ]
    }
   ],
   "source": [
    "print(main_similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie ID: 2365,  Predicted Rating: 0.05451489502638481\n",
      "Movie ID: 2492,  Predicted Rating: 0.040374468318392726\n",
      "Movie ID: 1673,  Predicted Rating: 0.03738946082324682\n",
      "Movie ID: 1894,  Predicted Rating: 0.03501843850974258\n",
      "Movie ID: 2534,  Predicted Rating: 0.03452330456139721\n",
      "Movie ID: 2459,  Predicted Rating: 0.033943952482750395\n",
      "Movie ID: 53,  Predicted Rating: 0.032299574654714176\n",
      "Movie ID: 2371,  Predicted Rating: 0.0317260095516319\n",
      "Movie ID: 2446,  Predicted Rating: 0.02919626375876557\n",
      "Movie ID: 783,  Predicted Rating: 0.028042095617435116\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "similarity_matrix = main_similarity_matrix\n",
    "user_id = '12'  # Specify the user ID for which you want to generate recommendations\n",
    "recommendations = recommend_movies2(user_id, Main_csr_matrix, similarity_matrix, N=10)\n",
    "\n",
    "# Print the recommendations\n",
    "for movie_id, predicted_rating in recommendations:\n",
    "    print(f\"Movie ID: {movie_id},  Predicted Rating: {predicted_rating}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
