{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>YearOfRelease</th>\n",
       "      <th>RatingYear</th>\n",
       "      <th>MovieAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1488844</td>\n",
       "      <td>3</td>\n",
       "      <td>2005-09-06</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>822109</td>\n",
       "      <td>5</td>\n",
       "      <td>2005-05-13</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>885013</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-10-19</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30878</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-12-26</td>\n",
       "      <td>2003</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>823519</td>\n",
       "      <td>3</td>\n",
       "      <td>2004-05-03</td>\n",
       "      <td>2003</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID  CustomerID  Rating        Date  YearOfRelease  RatingYear  \\\n",
       "0        1     1488844       3  2005-09-06           2003        2005   \n",
       "1        1      822109       5  2005-05-13           2003        2005   \n",
       "2        1      885013       4  2005-10-19           2003        2005   \n",
       "3        1       30878       4  2005-12-26           2003        2005   \n",
       "4        1      823519       3  2004-05-03           2003        2004   \n",
       "\n",
       "   MovieAge  \n",
       "0         2  \n",
       "1         2  \n",
       "2         2  \n",
       "3         2  \n",
       "4         1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "training_df = pd.read_csv('C:/Users/nafla/OneDrive/Documents/system development/Netflix/training_data.csv')\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25     8.0\n",
      "0.50    24.0\n",
      "0.75    64.0\n",
      "Name: CustomerID, dtype: float64\n",
      "0.25     192.0\n",
      "0.50     552.5\n",
      "0.75    2539.0\n",
      "Name: MovieID, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate quantiles for user activity and item popularity\n",
    "user_activity_quantiles = training_df['CustomerID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "item_popularity_quantiles = training_df['MovieID'].value_counts().quantile([0.25, 0.5, 0.75])\n",
    "print(user_activity_quantiles)\n",
    "print(item_popularity_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['CustomerID'] = training_df['CustomerID'].astype(str)\n",
    "training_df['MovieID'] = training_df['MovieID'].astype(str)\n",
    "training_df['Rating'] = pd.to_numeric(training_df['Rating'], errors='coerce')  # Converts to float, makes non-numeric as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before conversion:\n",
      "MovieID          object\n",
      "CustomerID       object\n",
      "Rating            int64\n",
      "Date             object\n",
      "YearOfRelease     int64\n",
      "RatingYear        int64\n",
      "MovieAge          int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(\"Before conversion:\")\n",
    "print(training_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Sampling Method \n",
    "\n",
    "To create a representative sample of our dataset, we employ a stratified sampling method that accounts for three key dimensions: Rating Distribution, User Activity, and Item Popularity. This approach ensures our sample maintains the diversity and characteristics of the entire dataset, facilitating more reliable model training and evaluation.\n",
    "\n",
    "- User Activity is quantified by the number of ratings a user has provided.\n",
    "- Item Popularity reflects the number of ratings an item has received.\n",
    "\n",
    "Finally, We combine User Activity, Item Popularity, and Rating into a composite stratification key for each record. This multi-dimensional key ensures our sampling process considers the distribution across all three axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each user and item to a bin based on the quantiles\n",
    "training_df['UserActivityBin'] = pd.qcut(training_df.groupby('CustomerID')['Rating'].transform('size'), \n",
    "                                q=[0, .25, .5, .75, 1], labels=['low', 'medium', 'medium-high', 'high'])\n",
    "\n",
    "training_df['ItemPopularityBin'] = pd.qcut(training_df.groupby('MovieID')['Rating'].transform('size'), \n",
    "                                  q=[0, .25, .5, .75, 1], labels=['low', 'medium', 'medium-high', 'high'])\n",
    "\n",
    "# Combine these with Rating to create a stratification key\n",
    "training_df['Strata'] = training_df['UserActivityBin'].astype(str) + \"_\" + training_df['ItemPopularityBin'].astype(str) + \"_\" + training_df['Rating'].astype(str)\n",
    "\n",
    "# Perform stratified sampling\n",
    "# we use groupby and  frac to specify a fraction of each strata and in case number of rows is less that 10 it takes all rows\n",
    "strat_sample_df = training_df.groupby('Strata').apply(lambda x: x.sample(frac=0.005 if len(x) > 10 else len(x)/len(x))).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the sampled DataFrame: 120269\n"
     ]
    }
   ],
   "source": [
    "num_sampled_rows = len(strat_sample_df)\n",
    "print(f\"Number of rows in the sampled DataFrame: {num_sampled_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset to training, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the remaining data into training, testing, and validation sets\n",
    "train, testing_data = train_test_split(strat_sample_df, test_size=0.2, random_state=42)\n",
    "training_data , validation_data = train_test_split(train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 76972\n",
      "Validation Data Size: 19243\n",
      "Testing Data Size: 24054\n"
     ]
    }
   ],
   "source": [
    "# Calculate the size of each split\n",
    "training_size = training_data.shape[0]  # Number of rows in the training data\n",
    "validation_size = validation_data.shape[0]  # Number of rows in the validation data\n",
    "testing_size = testing_data.shape[0]  # Number of rows in the testing data\n",
    "\n",
    "# Print the sizes\n",
    "print(f\"Training Data Size: {training_size}\")\n",
    "print(f\"Validation Data Size: {validation_size}\")\n",
    "print(f\"Testing Data Size: {testing_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique MovieIDs in Training Data: 3311\n",
      "Unique MovieIDs in Validation Data: 2124\n",
      "Unique MovieIDs in Testing Data: 2310\n"
     ]
    }
   ],
   "source": [
    "# Assuming final_training_data, validation_data, and testing_data are your data splits\n",
    "\n",
    "# Count unique MovieIDs in the final training data\n",
    "unique_movies_training = training_data['MovieID'].nunique()\n",
    "\n",
    "# Count unique MovieIDs in the validation data\n",
    "unique_movies_validation = validation_data['MovieID'].nunique()\n",
    "\n",
    "# Count unique MovieIDs in the testing data\n",
    "unique_movies_testing = testing_data['MovieID'].nunique()\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Unique MovieIDs in Training Data: {unique_movies_training}\")\n",
    "print(f\"Unique MovieIDs in Validation Data: {unique_movies_validation}\")\n",
    "print(f\"Unique MovieIDs in Testing Data: {unique_movies_testing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating User - Item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t5\n",
      "  (1, 1)\t4\n",
      "  (1, 130)\t5\n",
      "  (1, 477)\t4\n",
      "  (1, 779)\t5\n",
      "  (2, 2)\t2\n",
      "  (3, 3)\t2\n",
      "  (4, 4)\t3\n",
      "  (4, 110)\t4\n",
      "  (5, 5)\t4\n",
      "  (5, 699)\t4\n",
      "  (6, 6)\t5\n",
      "  (7, 7)\t5\n",
      "  (8, 8)\t3\n",
      "  (9, 9)\t4\n",
      "  (10, 10)\t5\n",
      "  (10, 175)\t3\n",
      "  (11, 11)\t4\n",
      "  (11, 770)\t4\n",
      "  (12, 12)\t1\n",
      "  (13, 13)\t4\n",
      "  (14, 14)\t4\n",
      "  (15, 15)\t3\n",
      "  (15, 387)\t3\n",
      "  (15, 646)\t5\n",
      "  :\t:\n",
      "  (62381, 232)\t3\n",
      "  (62382, 336)\t3\n",
      "  (62383, 225)\t5\n",
      "  (62384, 96)\t5\n",
      "  (62385, 41)\t5\n",
      "  (62386, 118)\t5\n",
      "  (62387, 348)\t5\n",
      "  (62388, 828)\t3\n",
      "  (62389, 702)\t4\n",
      "  (62390, 9)\t5\n",
      "  (62391, 118)\t3\n",
      "  (62392, 278)\t3\n",
      "  (62393, 434)\t5\n",
      "  (62394, 2062)\t3\n",
      "  (62395, 171)\t4\n",
      "  (62396, 151)\t2\n",
      "  (62397, 725)\t3\n",
      "  (62398, 1368)\t1\n",
      "  (62399, 1082)\t3\n",
      "  (62400, 433)\t4\n",
      "  (62401, 65)\t3\n",
      "  (62402, 142)\t5\n",
      "  (62403, 191)\t3\n",
      "  (62404, 89)\t4\n",
      "  (62405, 29)\t5\n"
     ]
    }
   ],
   "source": [
    "# Creating customer-movie matrix\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# Map user IDs and movie IDs to integer indices for CSR matrix\n",
    "user_ids = training_data['CustomerID'].unique()\n",
    "movie_ids = training_data['MovieID'].unique()\n",
    "\n",
    "user_id_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "movie_id_to_index = {movie_id: index for index, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "# Prepare row, column, and data arrays for csr_matrix\n",
    "rows = training_data['CustomerID'].map(user_id_to_index)\n",
    "cols = training_data['MovieID'].map(movie_id_to_index)\n",
    "data = training_data['Rating']\n",
    "\n",
    "# Create CSR matrix\n",
    "ratings_csr_matrix = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(movie_ids)))\n",
    "\n",
    "\n",
    "print(ratings_csr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define similarity function for each given user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_cosine_similarity(user_item_matrix, target_user_ratings):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity scores between a target user's ratings and all other users.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_item_matrix: DataFrame with users as rows, movies as columns, and ratings as values.\n",
    "    - target_user_ratings: Series containing the target user's ratings, indexed by MovieID.\n",
    "    \n",
    "    Returns:\n",
    "    A Series with user IDs as the index and the cosine similarity scores as the values.\n",
    "    \"\"\"\n",
    "    # Ensure target_user_ratings is a DataFrame row for compatibility with cosine_similarity\n",
    "    target_user_df = pd.DataFrame(target_user_ratings).T.fillna(0)\n",
    "    \n",
    "    # Align user_item_matrix with target_user_df to match columns (MovieIDs)\n",
    "    aligned_user_item_matrix = user_item_matrix.reindex(columns=target_user_df.columns, fill_value=0)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity(aligned_user_item_matrix, target_user_df)\n",
    "    \n",
    "    # Flatten the similarities array and create a Series with user IDs as index\n",
    "    similarities_series = pd.Series(similarities.flatten(), index=aligned_user_item_matrix.index)\n",
    "    \n",
    "    return similarities_series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def calculate_cosine_similarity_matrix_csr(user_item_matrix_csr):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity scores between all pairs of users using a CSR matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_item_matrix_csr: CSR matrix with users as rows, movies as columns, and ratings as values.\n",
    "    \n",
    "    Returns:\n",
    "    A CSR matrix representing the user-user similarity matrix with cosine similarity scores.\n",
    "    \"\"\"\n",
    "    # Calculate cosine similarities between all users\n",
    "    similarity_matrix_csr = cosine_similarity(user_item_matrix_csr, dense_output=False)\n",
    "    \n",
    "    return similarity_matrix_csr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# defining function to calculate pearson correlation for pair of users\n",
    "def calculate_pearson_similarity(user_item_matrix, target_user_ratings):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between a target user's ratings\n",
    "    and all other users' ratings in the user-item matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_item_matrix: DataFrame with users as rows, movies as columns, and ratings as values.\n",
    "    - target_user_ratings: Series containing the target user's ratings, indexed by MovieID.\n",
    "    \n",
    "    Returns:\n",
    "    - A Series with user IDs as the index and the Pearson correlation coefficients as the values.\n",
    "    \"\"\"\n",
    "    similarities = {}\n",
    "    \n",
    "    user_item_matrix_replaced = user_item_matrix.replace(0, np.nan)\n",
    "\n",
    "    # Loop through each user in the user-item matrix\n",
    "    for user_id, user_ratings in user_item_matrix_replaced.iterrows():\n",
    "        common_movies = user_ratings.dropna().index.intersection(target_user_ratings.dropna().index)\n",
    "        if len(common_movies) >= 2:\n",
    "            correlation, _ = pearsonr(user_ratings.loc[common_movies], target_user_ratings.loc[common_movies])\n",
    "            if np.isnan(correlation):\n",
    "                # Handle NaN correlation explicitly\n",
    "                similarities[user_id] = 0\n",
    "            elif np.isfinite(correlation):\n",
    "                similarities[user_id] = correlation\n",
    "        else:\n",
    "            similarities[user_id] = 0\n",
    "    # Convert the similarities dictionary to a pandas Series\n",
    "    similarity_series = pd.Series(similarities, name='Similarity').sort_values(ascending=False)\n",
    "    \n",
    "    return similarity_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to calculate manhattan distance for ratings of each pair of users\n",
    "def calculate_manhattan_similarity(user_item_matrix, target_user_ratings):\n",
    "    \"\"\"\n",
    "    Calculate user similarities using Manhattan distance, comparing target user's ratings\n",
    "    with those in the user_item_matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_item_matrix: DataFrame where rows are users, columns are items, and values are ratings (training data).\n",
    "    - target_user_ratings: Series or dict containing the target user's movie ratings.\n",
    "    \n",
    "    Returns:\n",
    "    A Series with user IDs as the index and the similarity scores as the values.\n",
    "    \"\"\"\n",
    "    similarities = {}\n",
    "\n",
    "    for user_id in user_item_matrix.index:\n",
    "        user_ratings = user_item_matrix.loc[user_id]\n",
    "        \n",
    "        # Calculate distance only for movies both have rated\n",
    "        common_movies = user_ratings.index.intersection(target_user_ratings.index)\n",
    "        if not common_movies.empty:\n",
    "            distance = np.nansum(np.abs(user_ratings[common_movies] - target_user_ratings[common_movies]))\n",
    "            similarity = 1 / (1 + distance) if distance != 0 else 0\n",
    "        else:\n",
    "            similarity = 0  # No common movies means no similarity\n",
    "        \n",
    "        similarities[user_id] = similarity\n",
    "\n",
    "    similarity_series = pd.Series(similarities, name=\"Similarity\").sort_values(ascending=False)\n",
    "    return similarity_series\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict ratings using similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to predict rating for each pair of user and movie based on their similarity\n",
    "def predict_rating(user_item_matrix, target_user_ratings, movie_id, k, similarity_method):\n",
    "\n",
    "    \"\"\"\n",
    "    Predict the rating for a given movie by a target user, based on the ratings of top-k similar users.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_item_matrix: DataFrame with users as rows, movies as columns, and ratings as values.\n",
    "    - target_user_id: The ID of the user for whom the rating is being predicted.\n",
    "    - movie_id: The ID of the movie for which the rating is being predicted.\n",
    "    - k: Number of top similar users to consider for prediction.\n",
    "    - similarity_method: Function to calculate similarity scores between users.\n",
    "    \n",
    "    Returns:\n",
    "    - Predicted rating for the movie by the target user.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Calculate similarity scores between the target user and all others\n",
    "    similarities = similarity_method(user_item_matrix, target_user_ratings)\n",
    "    \n",
    "   # Filter users who have rated the movie\n",
    "    users_who_rated_movie = user_item_matrix.index[user_item_matrix[movie_id].notnull()]\n",
    "    users_with_similarity_scores = similarities.index.intersection(users_who_rated_movie)\n",
    "\n",
    "    # Retain similarity scores for users who have rated the movie\n",
    "    similarities_filtered = similarities.loc[users_with_similarity_scores]\n",
    "    \n",
    "    # Filter top-k similar users from those who have rated the movie\n",
    "    top_k_users = similarities_filtered.nlargest(k).index\n",
    "    \n",
    "    # Retrieve ratings for the movie from these top-k similar users\n",
    "    top_k_ratings = user_item_matrix.loc[top_k_users, movie_id]\n",
    "    \n",
    " # Calculate weighted average rating\n",
    "    if not top_k_ratings.isnull().all():\n",
    "        weighted_ratings = top_k_ratings * similarities.loc[top_k_users]\n",
    "        predicted_rating = weighted_ratings.sum() / similarities.loc[top_k_users].sum()\n",
    "    else:\n",
    "        # Use the average rating for the movie if available\n",
    "        if user_item_matrix[movie_id].notnull().any():\n",
    "            predicted_rating = user_item_matrix[movie_id].mean()\n",
    "        else:\n",
    "            # Default rating if the movie has not been rated by anyone\n",
    "            predicted_rating = 2.5\n",
    "            \n",
    "    return predicted_rating\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating_with_similarity_matrix(user_item_matrix, similarity_matrix,user_index, movie_index, k):\n",
    "    \"\"\"\n",
    "    Predict the rating for a given movie by a target user, based on the ratings of top-k similar users.\n",
    "    This function uses a pre-calculated similarity matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_item_matrix: DataFrame with users as rows, movies as columns, and ratings as values.\n",
    "    - similarity_matrix: DataFrame representing the similarity scores between users.\n",
    "    - user_id: The ID of the user for whom the rating is being predicted.\n",
    "    - movie_id: The ID of the movie for which the rating is being predicted.\n",
    "    - k: Number of top similar users to consider for prediction.\n",
    "    \n",
    "    Returns:\n",
    "    - Predicted rating for the movie by the target user.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the movie is rated by anyone\n",
    "    if movie_index not in movie_id_to_index:\n",
    "        return 2.5  # Default rating if the movie has not been rated by anyone\n",
    "    \n",
    "    # Extract similarity scores for the target user\n",
    "    user_similarities = similarity_matrix[user_index, :]\n",
    "\n",
    "    # Extract ratings for the target movie\n",
    "    movie_ratings = user_item_matrix[:, movie_index].toarray().flatten()\n",
    "\n",
    "    # Filter users who have rated the movie\n",
    "    users_who_rated_movie = user_item_matrix.loc[user_item_matrix[movie_index].notnull()].index\n",
    "    \n",
    "    # Keep similarities only for users who have rated the movie\n",
    "    similarities_filtered = user_similarities[users_who_rated_movie]\n",
    "    \n",
    "    # Find indices of top-k similar users\n",
    "    top_k_indices = np.argsort(similarities_filtered)[-k:]\n",
    "    \n",
    "\n",
    "        # Compute the weighted average rating\n",
    "    weighted_sum = np.dot(user_similarities[top_k_indices], movie_ratings[top_k_indices])\n",
    "    similarity_sum = user_similarities[top_k_indices].sum()\n",
    "\n",
    "    if similarity_sum > 0:\n",
    "        predicted_rating = weighted_sum / similarity_sum\n",
    "    else:\n",
    "        predicted_rating = np.nan  # or 2.5, if you prefer a default rating\n",
    "    \n",
    "    return predicted_rating\n",
    "\n",
    "    # # Compute the weighted average rating\n",
    "    # weighted_sum = 0\n",
    "    # similarity_sum = 0\n",
    "    # for similar_user_id in top_k_indices:\n",
    "    #     # Skip if it's the same user\n",
    "    #     if similar_user_id == user_id:\n",
    "    #         continue\n",
    "    #     user_similarity_score = similarities_filtered[similar_user_id]\n",
    "    #     user_rating = user_item_matrix.loc[similar_user_id, movie_id]\n",
    "    #     weighted_sum += user_similarity_score * user_rating\n",
    "    #     similarity_sum += user_similarity_score\n",
    "\n",
    "    # # If there were any similar users who rated the movie, calculate the weighted average\n",
    "    # if similarity_sum > 0:\n",
    "    #     predicted_rating = weighted_sum / similarity_sum\n",
    "    # else:\n",
    "    #     # Default to the average rating for the movie\n",
    "    #     predicted_rating = user_item_matrix[movie_id].mean()\n",
    "\n",
    "    # return predicted_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# defining a function to evaluate accuracy of prediced ratings for each pair of user and movie\n",
    "def evaluate_predictions(validation_data, user_item_matrix, k, similarity_method):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendation system by predicting ratings for each user-movie pair in the validation set\n",
    "    and comparing the predictions to the actual ratings using RMSE.\n",
    "    \n",
    "    Parameters:\n",
    "    - validation_data: DataFrame containing 'CustomerID', 'MovieID', and 'Rating'.\n",
    "    - user_item_matrix: DataFrame representing the user-item matrix from the training set.\n",
    "    - k: The number of top similar users to consider when making predictions.\n",
    "    - similarity_method: The function to calculate similarity scores between users.\n",
    "    \n",
    "    Returns:\n",
    "    - rmse: The root mean square error of the predicted ratings against the actual ratings.\n",
    "    \"\"\"\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    # Your existing logic to populate actual_ratings and predicted_ratings\n",
    "\n",
    "\n",
    "    user_ratings_map = validation_data.groupby('CustomerID').apply(lambda x: x.set_index('MovieID')['Rating'])\n",
    "\n",
    "    # Iterate over each row in the validation data\n",
    "    # Inside evaluate_predictions, before calling predict_rating:\n",
    "    for _, row in validation_data.iterrows():\n",
    "        user_id = row['CustomerID']\n",
    "        movie_id = row['MovieID']\n",
    "        actual_rating = row['Rating']\n",
    "    \n",
    "    # Prepare target_user_ratings as a Series\n",
    "    # This assumes you have a way to extract all ratings for user_id from validation_data\n",
    "    # Here's a placeholder for how you might do this, replace with your actual logic\n",
    "        target_user_ratings = user_ratings_map.get(user_id, pd.Series(dtype='float64'))\n",
    "\n",
    "    # Now call predict_rating with target_user_ratings instead of target_user_id\n",
    "    if movie_id in user_item_matrix.columns and not target_user_ratings.empty:  # Check if movie exists in training data\n",
    "        predicted_rating = predict_rating(user_item_matrix, target_user_ratings, movie_id, k, similarity_method)\n",
    "        actual_ratings.append(actual_rating)\n",
    "        predicted_ratings.append(predicted_rating)\n",
    "        \n",
    "    \n",
    "    # Calculate RMSE between actual and predicted ratings\n",
    "    rmse = sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions_csr(validation_data, csr_user_item_matrix, similarity_matrix, k, user_id_to_index, movie_id_to_index):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendation system by predicting ratings for each user-movie pair in the validation set\n",
    "    using a CSR matrix and pre-computed similarity matrix, and comparing the predictions to the actual ratings using RMSE.\n",
    "\n",
    "    Parameters:\n",
    "    - validation_data: DataFrame containing 'UserIndex', 'MovieIndex', and 'Rating'.\n",
    "    - csr_user_item_matrix: CSR matrix representing the user-item matrix from the training set.\n",
    "    - similarity_matrix: Pre-computed similarity matrix as a CSR matrix.\n",
    "    - k: The number of top similar users to consider when making predictions.\n",
    "    - user_id_to_index: Dictionary mapping user IDs to indices.\n",
    "    - movie_id_to_index: Dictionary mapping movie IDs to indices.\n",
    "    \n",
    "    Returns:\n",
    "    - rmse: The root mean square error of the predicted ratings against the actual ratings.\n",
    "    \"\"\"\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for _, row in validation_data.iterrows():\n",
    "        user_index = row['UserIndex']\n",
    "        movie_index = row['MovieIndex']\n",
    "        actual_rating = row['Rating']\n",
    "        \n",
    "        # Check if the movie index is valid\n",
    "        if movie_index in csr_user_item_matrix.indices:\n",
    "            predicted_rating = predict_rating_with_similarity_matrix(csr_user_item_matrix, similarity_matrix, user_index, movie_index, k)\n",
    "            actual_ratings.append(actual_rating)\n",
    "            predicted_ratings.append(predicted_rating)\n",
    "    \n",
    "    # Calculate RMSE between actual and predicted ratings\n",
    "    actual_ratings = np.array(actual_ratings)\n",
    "    predicted_ratings = np.array(predicted_ratings)\n",
    "    valid_mask = ~np.isnan(predicted_ratings)\n",
    "    rmse = sqrt(mean_squared_error(actual_ratings[valid_mask], predicted_ratings[valid_mask]))\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ids_to_indices(validation_data, user_id_to_index, movie_id_to_index):\n",
    "    \"\"\"\n",
    "    Map user IDs and movie IDs to their respective indices.\n",
    "\n",
    "    Parameters:\n",
    "    - validation_data: DataFrame containing 'CustomerID', 'MovieID', and other columns.\n",
    "    - user_id_to_index: Dictionary mapping user IDs to indices.\n",
    "    - movie_id_to_index: Dictionary mapping movie IDs to indices.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with added columns 'UserIndex' and 'MovieIndex' for the respective indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy to avoid modifying the original DataFrame\n",
    "    modified_data = validation_data.copy()\n",
    "    \n",
    "    # Map 'CustomerID' to 'UserIndex'\n",
    "    modified_data['UserIndex'] = modified_data['CustomerID'].map(user_id_to_index)\n",
    "    \n",
    "    # Map 'MovieID' to 'MovieIndex'\n",
    "    modified_data['MovieIndex'] = modified_data['MovieID'].map(movie_id_to_index)\n",
    "    \n",
    "    # Optional: drop rows where either UserIndex or MovieIndex is NaN (i.e., ID wasn't found)\n",
    "    modified_data.dropna(subset=['UserIndex', 'MovieIndex'], inplace=True)\n",
    "    \n",
    "    # Convert indices to integers (they might be floats due to NaN handling)\n",
    "    modified_data['UserIndex'] = modified_data['UserIndex'].astype(int)\n",
    "    modified_data['MovieIndex'] = modified_data['MovieIndex'].astype(int)\n",
    "    \n",
    "    return modified_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_validation_data = map_ids_to_indices(validation_data, user_id_to_index, movie_id_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>YearOfRelease</th>\n",
       "      <th>RatingYear</th>\n",
       "      <th>MovieAge</th>\n",
       "      <th>UserActivityBin</th>\n",
       "      <th>ItemPopularityBin</th>\n",
       "      <th>Strata</th>\n",
       "      <th>UserIndex</th>\n",
       "      <th>MovieIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76827</th>\n",
       "      <td>1719</td>\n",
       "      <td>769555</td>\n",
       "      <td>3</td>\n",
       "      <td>2005-06-13</td>\n",
       "      <td>2004</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>medium-high</td>\n",
       "      <td>medium-high</td>\n",
       "      <td>medium-high_medium-high_3</td>\n",
       "      <td>3228</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23683</th>\n",
       "      <td>3890</td>\n",
       "      <td>682697</td>\n",
       "      <td>3</td>\n",
       "      <td>2004-12-10</td>\n",
       "      <td>2004</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>medium</td>\n",
       "      <td>high_medium_3</td>\n",
       "      <td>49282</td>\n",
       "      <td>828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78709</th>\n",
       "      <td>312</td>\n",
       "      <td>617941</td>\n",
       "      <td>4</td>\n",
       "      <td>2002-06-14</td>\n",
       "      <td>2000</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>medium-high</td>\n",
       "      <td>medium-high</td>\n",
       "      <td>medium-high_medium-high_4</td>\n",
       "      <td>39082</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15611</th>\n",
       "      <td>1843</td>\n",
       "      <td>2625306</td>\n",
       "      <td>2</td>\n",
       "      <td>2004-04-12</td>\n",
       "      <td>1994</td>\n",
       "      <td>2004</td>\n",
       "      <td>10</td>\n",
       "      <td>high</td>\n",
       "      <td>medium-high</td>\n",
       "      <td>high_medium-high_2</td>\n",
       "      <td>20580</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13050</th>\n",
       "      <td>4201</td>\n",
       "      <td>1295669</td>\n",
       "      <td>4</td>\n",
       "      <td>2002-10-05</td>\n",
       "      <td>1996</td>\n",
       "      <td>2002</td>\n",
       "      <td>6</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>high_low_4</td>\n",
       "      <td>4352</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MovieID CustomerID  Rating        Date  YearOfRelease  RatingYear  \\\n",
       "76827    1719     769555       3  2005-06-13           2004        2005   \n",
       "23683    3890     682697       3  2004-12-10           2004        2004   \n",
       "78709     312     617941       4  2002-06-14           2000        2002   \n",
       "15611    1843    2625306       2  2004-04-12           1994        2004   \n",
       "13050    4201    1295669       4  2002-10-05           1996        2002   \n",
       "\n",
       "       MovieAge UserActivityBin ItemPopularityBin                     Strata  \\\n",
       "76827         1     medium-high       medium-high  medium-high_medium-high_3   \n",
       "23683         0            high            medium              high_medium_3   \n",
       "78709         2     medium-high       medium-high  medium-high_medium-high_4   \n",
       "15611        10            high       medium-high         high_medium-high_2   \n",
       "13050         6            high               low                 high_low_4   \n",
       "\n",
       "       UserIndex  MovieIndex  \n",
       "76827       3228          88  \n",
       "23683      49282         828  \n",
       "78709      39082          56  \n",
       "15611      20580          31  \n",
       "13050       4352        1436  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4887757342686776\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "similarity_matrix = calculate_cosine_similarity_matrix_csr(ratings_csr_matrix)\n",
    "k = 10  # Example value for k\n",
    "rmse = evaluate_predictions_csr(mapped_validation_data, ratings_csr_matrix, similarity_matrix, k, user_id_to_index, movie_id_to_index)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for k=5: 1.4887757342686776\n",
      "RMSE for k=30: 1.4887757342686776\n",
      "RMSE for k=70: 1.4887757342686776\n",
      "RMSE for k=150: 1.4887757342686776\n",
      "RMSE for k=200: 1.4887757342686776\n",
      "RMSE for k=300: 1.4887757342686776\n",
      "RMSE for k=500: 1.4887757342686776\n",
      "\n",
      "Optimal k value: 5 with RMSE: 1.4887757342686776\n"
     ]
    }
   ],
   "source": [
    "# Define a range of k values to test\n",
    "k_values = [ 5, 30, 70, 150, 200, 300, 500]\n",
    "\n",
    "# Initialize a dictionary to store the RMSE for each k value\n",
    "k_rmse_results = {}\n",
    "\n",
    "# Loop over each k value\n",
    "for k in k_values:\n",
    "    # Evaluate the recommender system using the current k value\n",
    "    rmse = evaluate_predictions_csr(mapped_validation_data, ratings_csr_matrix, similarity_matrix, k, user_id_to_index, movie_id_to_index)\n",
    "\n",
    "    # Store the RMSE in the dictionary\n",
    "    k_rmse_results[k] = rmse\n",
    "    \n",
    "    # Print the result for the current k\n",
    "    print(f\"RMSE for k={k}: {rmse}\")\n",
    "\n",
    "# Identify the k value with the lowest RMSE\n",
    "optimal_k = min(k_rmse_results, key=k_rmse_results.get)\n",
    "optimal_rmse = k_rmse_results[optimal_k]\n",
    "\n",
    "print(f\"\\nOptimal k value: {optimal_k} with RMSE: {optimal_rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a dictionary to hold your similarity methods for easy access\n",
    "similarity_methods = {\n",
    "    'pearson': calculate_pearson_similarity,\n",
    "    'cosine': calculate_cosine_similarity,\n",
    "    'manhattan': calculate_manhattan_similarity  \n",
    "}\n",
    "\n",
    "# Define the range of k values you want to test\n",
    "k_values = range(5, 300, 20)\n",
    "\n",
    "# Placeholder for storing grid search results\n",
    "grid_search_results = []\n",
    "\n",
    "# Perform grid search\n",
    "for k in k_values:\n",
    "    for method_name, method_function in similarity_methods.items():\n",
    "        # Evaluate the recommender system's performance for each combination of k and similarity method\n",
    "        rmse = evaluate_predictions(validation_data, user_item_matrix, k, method_function)\n",
    "        \n",
    "        # Store the results\n",
    "        grid_search_results.append({'method': method_name, 'k': k, 'rmse': rmse})\n",
    "        \n",
    "        # Optionally print the results for each iteration\n",
    "        print(f\"Evaluated {method_name} method with k={k}: RMSE = {rmse}\")\n",
    "\n",
    "# Find the best performing combination of k and similarity method based on RMSE\n",
    "best_configuration = min(grid_search_results, key=lambda x: x['rmse'])\n",
    "\n",
    "# Output the best combination found\n",
    "print(f\"Best Configuration: Method = {best_configuration['method']}, k = {best_configuration['k']}, RMSE = {best_configuration['rmse']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def calculate_pearson_similarity_matrix_sparse(user_item_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between all pairs of users' ratings in the user-item matrix.\n",
    "    Only non-zero similarities are stored in a sparse matrix, making it more memory-efficient.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_item_matrix: DataFrame with users as rows, movies as columns, and ratings as values.\n",
    "    \n",
    "    Returns:\n",
    "    - similarity_matrix_sparse: A sparse matrix representing the user-user similarity matrix using Pearson correlation coefficients.\n",
    "    \"\"\"\n",
    "    # Replace 0 with NaN to ignore unrated items in the correlation computation\n",
    "    user_item_matrix_replaced = user_item_matrix.replace(0, np.nan)\n",
    "    \n",
    "    # Initialize a sparse matrix for storing similarities\n",
    "    n_users = user_item_matrix_replaced.shape[0]\n",
    "    similarity_matrix_sparse = lil_matrix((n_users, n_users))\n",
    "    \n",
    "    user_indices = {user_id: index for index, user_id in enumerate(user_item_matrix_replaced.index)}\n",
    "    \n",
    "    # Loop through each pair of users to calculate similarity\n",
    "    for user1 in user_item_matrix_replaced.index:\n",
    "        for user2 in user_item_matrix_replaced.index:\n",
    "            if user1 != user2:\n",
    "                user1_ratings = user_item_matrix_replaced.loc[user1]\n",
    "                user2_ratings = user_item_matrix_replaced.loc[user2]\n",
    "                \n",
    "                # Find common movies rated by both users\n",
    "                common_movies = user1_ratings.dropna().index.intersection(user2_ratings.dropna().index)\n",
    "                \n",
    "                if len(common_movies) >= 2:\n",
    "                    # Calculate Pearson correlation for common rated movies\n",
    "                    correlation, _ = pearsonr(user1_ratings.loc[common_movies], user2_ratings.loc[common_movies])\n",
    "                    \n",
    "                    # Store the correlation if it's valid and non-zero\n",
    "                    if np.isfinite(correlation) and correlation != 0:\n",
    "                        similarity_matrix_sparse[user_indices[user1], user_indices[user2]] = correlation\n",
    "                # No need to explicitly set values for pairs with fewer than 2 common ratings or for NaN correlations\n",
    "            else:\n",
    "                # Set self-similarity to 1\n",
    "                similarity_matrix_sparse[user_indices[user1], user_indices[user2]] = 1\n",
    "                \n",
    "    return similarity_matrix_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = calculate_pearson_similarity_matrix_sparse (user_item_matrix)\n",
    "similarity_matrix.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
